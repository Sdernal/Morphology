{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Working copy of SOTA POS-TAG with dataloader.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlCyLDbYgg62",
    "outputId": "2b4e4511-8813-4098-ccfc-17a90d68d0b7"
   },
   "source": [
    "!git clone https://github.com/Ryzhtus/morphology-homeworks"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cloning into 'abbyy-nlp-course'...\n",
      "remote: Enumerating objects: 17, done.\u001B[K\n",
      "remote: Total 17 (delta 0), reused 0 (delta 0), pack-reused 17\u001B[K\n",
      "Unpacking objects: 100% (17/17), done.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mB48fOSFv7T7",
    "outputId": "e3757354-4d20-4799-adaf-2b343dd20761"
   },
   "source": [
    "!pip install navec"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting navec\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/c1/771ec5565f0ce24874d7fd325b429f9caa80517a40d2e4ce5705120591f3/navec-0.10.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from navec) (1.18.5)\n",
      "Installing collected packages: navec\n",
      "Successfully installed navec-0.10.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LgRn9eFYlrE9"
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from navec import Navec\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "InZmKhUvqynt"
   },
   "source": [
    "class Sentence():\n",
    "    def __init__(self):\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "        self.grams = []"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Z6A3salirMZ8"
   },
   "source": [
    "def read_dataset(dataset, mode):\n",
    "    sentences = []\n",
    "    \n",
    "    with open(dataset, mode='r', encoding='utf-8') as data:\n",
    "        # Пропускаем заголовок\n",
    "        next(data)\n",
    "        \n",
    "        sentence = Sentence() # будем заполнять список предложений\n",
    "        \n",
    "        for row in data:\n",
    "            row = row.strip()\n",
    "            if len(row) != 0:\n",
    "                row = row.split('\\t')\n",
    "\n",
    "                if mode == 'train':\n",
    "                    _, _, token, pos_gram = row \n",
    "                    pos, gram = pos_gram.split('#')\n",
    "\n",
    "                else:\n",
    "                    _, _, token = row\n",
    "                    pos, gram = '<UNK>', '<UNK>'\n",
    "\n",
    "                sentence.tokens.append(token)\n",
    "                sentence.pos_tags.append(pos)\n",
    "                sentence.grams.append(gram)\n",
    "\n",
    "            else:\n",
    "                if len(sentence.tokens) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = Sentence()\n",
    "                \n",
    "        if len(sentence.tokens) > 0:\n",
    "            sentence.append(sentence)\n",
    "            \n",
    "    return sentences"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3uaccMRJrN8q"
   },
   "source": [
    "train = read_dataset('/content/morphology-homeworks/pos-tagging/data/train.csv', 'train')\n",
    "test = read_dataset('/content/morphology-homeworks/pos-tagging/data/test.csv', 'test')"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oEoAYzMbrQEE"
   },
   "source": [
    "def get_vocabulary_and_indexer(data):\n",
    "    vocabulary = set()\n",
    "    vocabulary.add('<PAD>')\n",
    "    \n",
    "    token2index = {}\n",
    "    index2token = {}\n",
    "    \n",
    "    token2index['<PAD>'] = 0\n",
    "    index2token[0] = '<PAD>'\n",
    "    index = 1\n",
    "    \n",
    "    for sentence in data:\n",
    "        for token in sentence.tokens:\n",
    "            vocabulary.add(token)\n",
    "    \n",
    "    for token in vocabulary:\n",
    "        if token == '<PAD>':\n",
    "              pass\n",
    "        else:\n",
    "            token2index[token] = index\n",
    "            index2token[index] = token\n",
    "            index += 1\n",
    "\n",
    "    return vocabulary, token2index, index2token\n",
    "\n",
    "def get_tags_and_indexer(data):\n",
    "    tags = set()\n",
    "    tags.add('<PAD>')\n",
    "    \n",
    "    tag2index = {}\n",
    "    tag2index['<PAD>'] = 0\n",
    "    \n",
    "    index2tag = {}\n",
    "    index2tag[0] = '<PAD>'\n",
    "\n",
    "    index = 1\n",
    "    for sentence in data:\n",
    "        for tag in sentence.pos_tags:\n",
    "            tags.add(tag)\n",
    "\n",
    "    for tag in tags:\n",
    "        if tag == '<PAD>':\n",
    "            pass\n",
    "        else:\n",
    "            tag2index[tag] = index\n",
    "            index2tag[index] = tag \n",
    "            index += 1\n",
    "\n",
    "    return tags, tag2index, index2tag\n",
    "\n",
    "def get_chars_and_indexer(data):\n",
    "    chars = set()\n",
    "    chars.add('<PAD>')\n",
    "    indexer = {}\n",
    "    indexer['<PAD>'] = 0\n",
    "    index = 1\n",
    "    for sentence in data:\n",
    "        for token in sentence.tokens:\n",
    "            for char in token:\n",
    "                chars.add(char)\n",
    "                \n",
    "    for char in chars:     \n",
    "        if char == '<PAD>':\n",
    "            pass\n",
    "        else:   \n",
    "            indexer[char] = index\n",
    "            index += 1\n",
    "\n",
    "    return chars, indexer"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HJFcjq-ccxin"
   },
   "source": [
    "train_vocab, train_token_index, train_index_token = get_vocabulary_and_indexer(train)\r\n",
    "train_tags, train_tag_index, train_index_tag = get_tags_and_indexer(train)\r\n",
    "train_chars, train_char_index = get_chars_and_indexer(train)\r\n",
    "\r\n",
    "test_vocab, test_token_index, test_index_token = get_vocabulary_and_indexer(test)\r\n",
    "test_tags, test_tag_index, test_index_tag = get_tags_and_indexer(test)\r\n",
    "test_chars, test_char_index = get_chars_and_indexer(test)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y-qBhmEbsQ4f"
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, device, token_indexer, tag_indexer, char_indexer):\n",
    "        self.sentences = sentences\n",
    "        self.max_pad_length = 200\n",
    "        self.device = device\n",
    "        self.token_indexer = token_indexer\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.char_indexer = char_indexer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.get_tensor_from_token(self.sentences[item]), self.get_tensor_from_tag(self.sentences[item]), self.get_tensor_from_char(self.sentences[item])\n",
    "\n",
    "    def get_tensor_from_token(self, sentence):\n",
    "        indicies = [self.token_indexer.get(token) for token in sentence.tokens]\n",
    "        if len(indicies) < self.max_pad_length:\n",
    "            indicies += [0] * (self.max_pad_length - len(indicies))\n",
    "        else:\n",
    "            indicies = indicies[:self.max_pad_length] \n",
    "\n",
    "        return torch.LongTensor(indicies)\n",
    "\n",
    "    def get_tensor_from_tag(self, sentence):\n",
    "        indicies = [self.tag_indexer.get(tag) for tag in sentence.pos_tags]\n",
    "        if len(indicies) < self.max_pad_length:\n",
    "            indicies += [0] * (self.max_pad_length - len(indicies))\n",
    "        else:\n",
    "            indicies = indicies[:self.max_pad_length] \n",
    "\n",
    "        return torch.LongTensor(indicies)\n",
    "\n",
    "    def get_tensor_from_char(self, sentence):\n",
    "        indicies = []\n",
    "        char_indicies = []\n",
    "        for token in sentence.tokens:\n",
    "            for char in token:\n",
    "                indicies.append(self.char_indexer.get(char))\n",
    "        \n",
    "        if len(indicies) < self.max_pad_length:\n",
    "            indicies += [0] * (self.max_pad_length - len(indicies))\n",
    "        else:\n",
    "            indicies = indicies[:self.max_pad_length] \n",
    "\n",
    "        return torch.LongTensor(indicies)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XsYfzg5UvaD5"
   },
   "source": [
    "dev = train[30000: 40000]\n",
    "train = train[:30000]"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dDX_DhpOvc40"
   },
   "source": [
    "TrainDataset = CustomDataset(train, device, train_token_index, train_tag_index, train_char_index)\n",
    "TrainDataloader = DataLoader(TrainDataset, 16)\n",
    "\n",
    "DevDataset = CustomDataset(dev, device, train_token_index, train_tag_index, train_char_index)\n",
    "DevDataloader = DataLoader(DevDataset, 16)\n",
    "\n",
    "TestDataset = CustomDataset(test, device, test_token_index, test_tag_index, test_char_index)\n",
    "TestDataloader = DataLoader(TestDataset, 16)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8LG8Xxagv4aP"
   },
   "source": [
    "path = '/content/abbyy-nlp-course/navec_hudlit_v1_12B_500K_300d_100q.tar'\r\n",
    "navec = Navec.load(path)\r\n",
    "\r\n",
    "WORD_EMBEDDING_DIM = 300\r\n",
    "\r\n",
    "embeddings = np.zeros((len(train_vocab), WORD_EMBEDDING_DIM))\r\n",
    "for idx, word in enumerate(train_vocab):\r\n",
    "    word = word.lower()\r\n",
    "    if word in navec:\r\n",
    "        embeddings[idx] = navec[word]"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lpuLFejLvmKM"
   },
   "source": [
    "class DualLSTMTagger(nn.Module):\n",
    "    def __init__(self, embeddings, word_embedding_dim, word_hidden_dim, char_embedding_dim, char_hidden_dim, word_vocab_size, char_vocab_size, tag_vocab_size):\n",
    "        super(DualLSTMTagger, self).__init__()\n",
    "\n",
    "        self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings), freeze=False)\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(word_embedding_dim + char_hidden_dim, word_hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(word_hidden_dim * 2, tag_vocab_size)\n",
    "        \n",
    "    def forward(self, words, chars):\n",
    "        word_embeddings = self.word_embedding(words)\n",
    "        char_embeddings = self.char_embedding(chars)\n",
    "\n",
    "        char_lstm_out, (char_lstm_hidden, char_cell_hidden) = self.char_lstm(char_embeddings)\n",
    "\n",
    "        combined = torch.cat((word_embeddings, char_lstm_out), 2)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(combined)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "      \n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        \n",
    "        return tag_scores"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "o7UDAQX14-OR"
   },
   "source": [
    "def remove_predictions_for_masked_items(predicted_labels, correct_labels): \r\n",
    "    predicted_labels_without_mask = []\r\n",
    "    correct_labels_without_mask = []\r\n",
    "    predicted_labels_one_array = []\r\n",
    "    correct_labels_one_array = []\r\n",
    "    for i in range(len(predicted_labels)):\r\n",
    "        predicted_labels_one_array += list(predicted_labels[i])\r\n",
    "        correct_labels_one_array += list(correct_labels[i])\r\n",
    "\r\n",
    "    for p, c in zip(predicted_labels_one_array, correct_labels_one_array):\r\n",
    "        if c > 1:\r\n",
    "            predicted_labels_without_mask.append(p)\r\n",
    "            correct_labels_without_mask.append(c)\r\n",
    "            \r\n",
    "    return predicted_labels_without_mask, correct_labels_without_mask"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sAgbRNZSyNI9"
   },
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, train_iterator, dev_iterator, lr=2e-5, device=device):\n",
    "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.train_iterator = train_iterator\n",
    "        self.dev_iterator = dev_iterator\n",
    "        self.model = model.to(device)\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        f1_epoch = 0\n",
    "        for batch_idx, (tokens, pos_tags, chars) in enumerate(self.train_iterator):\n",
    "            self.optimizer.zero_grad()\n",
    "            tokens = tokens.to(device)\n",
    "            chars = chars.to(device)\n",
    "            pos_tags = pos_tags.to(device)\n",
    "\n",
    "            logits = self.model(tokens, chars)\n",
    "            loss = self.criterion(logits.view(-1, logits.size(-1)), pos_tags.view(-1))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            mask = (tokens != 0).to(torch.long)\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "            correct += ((pred == pos_tags)*mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "\n",
    "            predict_label = list(pred.cpu().numpy())\n",
    "            correct_label = list(pos_tags.cpu().numpy())\n",
    "\n",
    "            predict_label, correct_label = remove_predictions_for_masked_items(predict_label, correct_label)\n",
    "            f1_batch = f1_score(predict_label, correct_label, average=\"micro\")\n",
    "            f1_epoch += f1_batch\n",
    "\n",
    "            if (batch_idx + 1) % 400 == 0:\n",
    "                print('Train-Batch-Loss: {:.4f}, Accuracy {:.4f}, F1-Score {:.4f}, Batch: {}/{}'.format(total_loss / (batch_idx + 1), correct / total, \n",
    "                                                                                                    f1_batch, batch_idx + 1, len(self.train_iterator)))\n",
    "        print('-' * 10)\n",
    "        print('Train-Loss: {:.4f}, Accuracy {:.4f}, F1-Score {:.4f}'.format(total_loss / (batch_idx + 1), correct / total, f1_epoch / len(self.train_iterator)))\n",
    "\n",
    "    def test_epoch(self):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            total_loss = 0\n",
    "            total = 0\n",
    "            correct = 0\n",
    "            f1_epoch = 0\n",
    "            for batch_idx, (tokens, pos_tags, chars) in enumerate(self.dev_iterator):\n",
    "                tokens = tokens.to(device)\n",
    "                chars = chars.to(device)\n",
    "                pos_tags = pos_tags.to(device)\n",
    "                logits = self.model(tokens, chars)\n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), pos_tags.view(-1))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                mask = (tokens != 0).to(torch.long)\n",
    "                pred = torch.argmax(logits, dim=-1)\n",
    "                correct += ((pred == pos_tags) * mask).sum().item()\n",
    "                total += mask.sum().item()\n",
    "\n",
    "                predict_label = list(pred.cpu().numpy())\n",
    "                correct_label = list(pos_tags.cpu().numpy())\n",
    "\n",
    "                predict_label, correct_label = remove_predictions_for_masked_items(predict_label, correct_label)\n",
    "                f1_batch = f1_score(predict_label, correct_label, average=\"micro\")\n",
    "                f1_epoch += f1_batch\n",
    "\n",
    "            print('-' * 10)\n",
    "            print('Eval-Loss: {:.4f}, Accuracy {:.4f}, F1-Score {:.4f}'.format(total_loss / (batch_idx + 1), correct / total, f1_epoch / len(self.dev_iterator)))"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O0DGStt7yVcz"
   },
   "source": [
    "WORD_EMBEDDING_DIM = 300\n",
    "CHAR_EMBEDDING_DIM = 128\n",
    "WORD_HIDDEN_DIM = 1024\n",
    "CHAR_HIDDEN_DIM = 1024\n",
    "EPOCHS = 70\n",
    "\n",
    "model = DualLSTMTagger(embeddings, WORD_EMBEDDING_DIM, WORD_HIDDEN_DIM, CHAR_EMBEDDING_DIM, CHAR_HIDDEN_DIM, len(train_vocab), len(train_chars), len(train_tags))\n",
    "trainer = Trainer(model, TrainDataloader, DevDataloader)"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6Hj5_IcZy0Ni",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef2f1c37-3f22-404c-fc12-4e4c67741168"
   },
   "source": [
    "for epoch in tqdm(range(10)):\r\n",
    "    print()\r\n",
    "    trainer.train_epoch()\r\n",
    "    trainer.test_epoch()"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/10 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "Train-Batch-Loss: 0.8850, Accuracy 0.1599, F1-Score 0.2636, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.5600, Accuracy 0.1861, F1-Score 0.3067, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.4375, Accuracy 0.2311, F1-Score 0.4458, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.3715, Accuracy 0.2759, F1-Score 0.4938, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.3397, Accuracy 0.3010, F1-Score 0.3374\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 10%|█         | 1/10 [05:48<52:19, 348.86s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.1516, Accuracy 0.4664, F1-Score 0.5223\n",
      "\n",
      "Train-Batch-Loss: 0.1462, Accuracy 0.4958, F1-Score 0.5969, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.1384, Accuracy 0.5189, F1-Score 0.5879, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.1318, Accuracy 0.5371, F1-Score 0.6750, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.1262, Accuracy 0.5540, F1-Score 0.7068, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.1225, Accuracy 0.5639, F1-Score 0.6267\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 20%|██        | 2/10 [11:59<47:23, 355.38s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0996, Accuracy 0.6341, F1-Score 0.6934\n",
      "\n",
      "Train-Batch-Loss: 0.0969, Accuracy 0.6477, F1-Score 0.7132, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0930, Accuracy 0.6581, F1-Score 0.6645, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0899, Accuracy 0.6678, F1-Score 0.7500, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0873, Accuracy 0.6774, F1-Score 0.7747, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0856, Accuracy 0.6824, F1-Score 0.7312\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 30%|███       | 3/10 [18:08<41:56, 359.55s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0753, Accuracy 0.7147, F1-Score 0.7524\n",
      "\n",
      "Train-Batch-Loss: 0.0738, Accuracy 0.7241, F1-Score 0.7907, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0718, Accuracy 0.7276, F1-Score 0.7125, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0703, Accuracy 0.7317, F1-Score 0.8000, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0691, Accuracy 0.7357, F1-Score 0.7840, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0682, Accuracy 0.7379, F1-Score 0.7708\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 40%|████      | 4/10 [24:18<36:15, 362.65s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0640, Accuracy 0.7510, F1-Score 0.7763\n",
      "\n",
      "Train-Batch-Loss: 0.0626, Accuracy 0.7607, F1-Score 0.8256, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0615, Accuracy 0.7626, F1-Score 0.7316, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0605, Accuracy 0.7647, F1-Score 0.8125, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0599, Accuracy 0.7672, F1-Score 0.7994, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0593, Accuracy 0.7683, F1-Score 0.7916\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 50%|█████     | 5/10 [30:30<30:26, 365.33s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0576, Accuracy 0.7725, F1-Score 0.7918\n",
      "\n",
      "Train-Batch-Loss: 0.0559, Accuracy 0.7827, F1-Score 0.8372, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0551, Accuracy 0.7842, F1-Score 0.7572, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0544, Accuracy 0.7857, F1-Score 0.8125, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0539, Accuracy 0.7878, F1-Score 0.8210, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0535, Accuracy 0.7886, F1-Score 0.8070\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 60%|██████    | 6/10 [36:41<24:28, 367.10s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0530, Accuracy 0.7891, F1-Score 0.8050\n",
      "\n",
      "Train-Batch-Loss: 0.0510, Accuracy 0.8005, F1-Score 0.8450, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0503, Accuracy 0.8017, F1-Score 0.7732, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0497, Accuracy 0.8031, F1-Score 0.8125, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0493, Accuracy 0.8048, F1-Score 0.8210, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0490, Accuracy 0.8056, F1-Score 0.8210\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 70%|███████   | 7/10 [42:50<18:23, 367.81s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0494, Accuracy 0.8034, F1-Score 0.8167\n",
      "\n",
      "Train-Batch-Loss: 0.0469, Accuracy 0.8162, F1-Score 0.8527, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0464, Accuracy 0.8170, F1-Score 0.7891, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0459, Accuracy 0.8182, F1-Score 0.8167, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0456, Accuracy 0.8197, F1-Score 0.8302, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0453, Accuracy 0.8205, F1-Score 0.8337\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 80%|████████  | 8/10 [49:00<12:16, 368.49s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0465, Accuracy 0.8153, F1-Score 0.8271\n",
      "\n",
      "Train-Batch-Loss: 0.0436, Accuracy 0.8299, F1-Score 0.8721, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0431, Accuracy 0.8302, F1-Score 0.8019, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0426, Accuracy 0.8311, F1-Score 0.8167, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0424, Accuracy 0.8324, F1-Score 0.8549, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0421, Accuracy 0.8330, F1-Score 0.8443\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r 90%|█████████ | 9/10 [55:14<06:09, 369.96s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0441, Accuracy 0.8242, F1-Score 0.8349\n",
      "\n",
      "Train-Batch-Loss: 0.0407, Accuracy 0.8408, F1-Score 0.8837, Batch: 400/1875\n",
      "Train-Batch-Loss: 0.0403, Accuracy 0.8411, F1-Score 0.8147, Batch: 800/1875\n",
      "Train-Batch-Loss: 0.0399, Accuracy 0.8418, F1-Score 0.8333, Batch: 1200/1875\n",
      "Train-Batch-Loss: 0.0397, Accuracy 0.8430, F1-Score 0.8704, Batch: 1600/1875\n",
      "----------\n",
      "Train-Loss: 0.0395, Accuracy 0.8436, F1-Score 0.8536\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:01:25<00:00, 368.55s/it]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "----------\n",
      "Eval-Loss: 0.0421, Accuracy 0.8318, F1-Score 0.8414\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    }
   ]
  }
 ]
}