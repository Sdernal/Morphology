{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфология 4\n",
    "В этом ноутбуке описана подготовка данных для задачи POS-tagging. А также пара простых моделей на keras, решающих данную задачу. Оригинальная задача и ноутбук есть в контесте: https://www.kaggle.com/c/rupos2018/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Загрузка корпуса\n",
    "Здесь мы прочитаем корпуса из csv и разложим их по спискам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ad156ee50218601b689e546907d0ce3109e155bd"
   },
   "outputs": [],
   "source": [
    "# Имена файлов с данными.\n",
    "TRAIN_FILENAME = \"data/train.csv\"\n",
    "TEST_FILENAME = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "fdbea001cf6b2aa7b763b4836050dc4fdabb6457"
   },
   "outputs": [],
   "source": [
    "# Считывание файлов.\n",
    "from collections import namedtuple\n",
    "WordForm = namedtuple(\"WordForm\", \"word pos gram\")\n",
    "\n",
    "def get_sentences(filename, is_train):\n",
    "    sentences = []\n",
    "    with open(filename, \"r\") as r:\n",
    "        # Пропускаем заголовок\n",
    "        next(r)\n",
    "        sentence = [] # будем заполнять список предложений\n",
    "        for line in r:\n",
    "            # предложения отделены по '\\n'\n",
    "            if len(line.strip()) == 0:\n",
    "                if len(sentence) == 0:\n",
    "                    continue\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            if is_train:\n",
    "                # Формат: индекс\\tномер_в_предложении\\tсловоформа\\tPOS#Грамемы\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                pos = line.strip().split(\"\\t\")[3].split(\"#\")[0]\n",
    "                gram = line.strip().split(\"\\t\")[3].split(\"#\")[1]\n",
    "                sentence.append(WordForm(word, pos, gram))\n",
    "            else:\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                sentence.append(word)\n",
    "        if len(sentence) != 0:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "fe4e1691bc5ef95abc711c5f403b27b897647878",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = get_sentences(TRAIN_FILENAME, True)\n",
    "test = get_sentences(TEST_FILENAME, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А \t CONJ \t _\n",
      "ведь \t PART \t _\n",
      "для \t ADP \t _\n",
      "конкретных \t ADJ \t Case=Gen|Degree=Pos|Number=Plur\n",
      "изделий \t NOUN \t Animacy=Inan|Case=Gen|Gender=Neut|Number=Plur\n",
      "зачастую \t ADV \t Degree=Pos\n",
      "нужен \t ADJ \t Degree=Pos|Gender=Masc|Number=Sing|Variant=Brev\n",
      "монокристалл \t NOUN \t Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "не \t PART \t _\n",
      "только \t PART \t _\n"
     ]
    }
   ],
   "source": [
    "# Выыедем, что получилось\n",
    "for wordform in train[0][:10]:\n",
    "    print(wordform.word, '\\t', wordform.pos, '\\t', wordform.gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты далее будем использовать токены слов и POS-теги. Но чтобы определять грамматические значения нужно еще провести некоторые манипуляции с данными, описанные в оригинальном ноутубке. Мы же ограничимся только определением частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Подготовка эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно в качестве признаков для обучения сеток используются словные эмбеддинги. Для этого можно скачать предобученные и сохранить их в матрицу, где в расположатся векторы эмбеддингах по индексам, соответсвующих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запомним все уникальные слова и POS-теги в корпусе\n",
    "word_set = set()\n",
    "pos_set = set()\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        word_set.add(wordform.word.lower())\n",
    "        pos_set.add(wordform.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "княжества, 1941, опровержения, актового, глоток, скромности, уклонялся, играй, отмыва, духов, \n",
      "{'VERB', 'ADP', 'X', 'SCONJ', 'NUM', 'ADJ', 'CONJ', 'NOUN', 'PUNCT', 'DET', 'SYM', 'PART', 'AUX', 'INTJ', 'ADV', 'PRON', 'PROPN'}\n"
     ]
    }
   ],
   "source": [
    "for word in list(word_set)[:10]: \n",
    "    print(word, end=', ')\n",
    "print()\n",
    "print(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузите эмбеддинги c https://nlp.stanford.edu/projects/glove/ или другие, которые вам нравятся и пропишите путь к ним\n",
    "import numpy as np\n",
    "\n",
    "word_embeddings_path = 'data/glove.twitter.27B.50d.txt'\n",
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "embedding_size = None\n",
    "#Загружаем эмбеддинги\n",
    "with open(word_embeddings_path, 'r') as f_em:\n",
    "    for line in f_em:\n",
    "        split = line.strip().split(\" \")\n",
    "        # Совсем короткие строки пропускаем\n",
    "        if len(split) <= 2:\n",
    "            continue\n",
    "        # Встретив первую подходящую строку, фиксируем размер эмбеддингов\n",
    "        if embedding_size is None:\n",
    "            embedding_size = len(split) - 1\n",
    "            # Также нициализируем эмбеддинги для паддингов и неизвестных слов\n",
    "            word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "            word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "        # После этого все эмбеддинги должны быть одинаковой длины\n",
    "        if len(split) - 1 != embedding_size:\n",
    "            continue\n",
    "            \n",
    "        #Если слова нет в корпусе, то не будем для него запоминать эмбеддинг        \n",
    "        if (split[0] not in word_set):\n",
    "            continue\n",
    "\n",
    "        word_embeddings.append(np.asarray(split[1:], dtype='float32'))\n",
    "        word2idx[split[0]] = len(word2idx)\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32402"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32400"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set & set(word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98880"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то эмбеддинги не сильно подходят для данного корпуса поэтому, просто инициализируем рандмно матрицу эмбеддингов при определении сетки. Вам же предлагается все-таки поискать подходящие эмбеддинги и использовать их при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Подготовка данных\n",
    "Теперь нам остается только пронумеровать все слова и POS-теги и можно переходить к обучению сеток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {'PAD' : 0, 'UNK' : 1}\n",
    "for word in word_set:\n",
    "    word_to_index[word] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.zeros([len(word_to_index), 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(word_to_index):\n",
    "    if word in word2idx:\n",
    "        embs[i] = word_embeddings[word2idx[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_index = {}\n",
    "index_to_pos = {}\n",
    "for pos in pos_set:\n",
    "    pos_to_index[pos] = len(pos_to_index)\n",
    "    index_to_pos[len(index_to_pos)] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для полносвязной сетки просто захреначим все в один список\n",
    "data_X = []\n",
    "data_Y = []\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        data_X.append(word_to_index[wordform.word.lower()])\n",
    "        data_Y.append(pos_to_index[wordform.pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[596, 74285, 94677, 55084, 58994, 70371, 92183, 26415, 87672, 2463]\n",
      "[6, 11, 1, 5, 7, 14, 5, 7, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "print(data_X[:10])\n",
    "print(data_Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Полносвязная сеть\n",
    "Самой простой моделью является обычный перцептрон. На вход сетки будем подавать просто эмдеддинг каждого слова, на выходе ожидать распредедение вероятностей по тегам. В качестве фреймворка достаточно будет использовать keras и его Sequential модель (https://keras.io/models/sequential/), в которую слои добавляются последовательно, с помощью метода `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 1, 50)             4944100   \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 17)                1717      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 4,950,917\n",
      "Trainable params: 4,950,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# на самом деле на вход сетки будет добавляться индекс слова, а слой эмбеддинга будет возвращать для него вектор\n",
    "model.add(Embedding(input_length=1, input_dim=len(word_to_index), output_dim=50, embeddings_initializer='random_uniform',\n",
    "                    )) # матрицу эмбеддингов просто инициализируем нормальным распределением и отключим обучение\n",
    "# далее нам нужно схлопнуть трехмерный тензор с одной фиктивной размерностью в двумерный\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100)) # основной полносвязный слой\n",
    "model.add(Activation('relu')) # для приличия добавим функцию активации\n",
    "model.add(Dense(len(pos_to_index))) # выходной слой тоже полносвязный размерности по кол-ву тегов\n",
    "model.add(Activation('softmax')) # ну и в конце делаем softmax, чтобы получить распределение\n",
    "model.summary() # вывод получившейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X_train, data_X_test, data_Y_train, data_Y_test = train_test_split(data_X, data_Y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# компилируем модель\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2493/2493 [==============================] - 82s 33ms/step - loss: 0.4415 - accuracy: 0.8481 - val_loss: 0.2448 - val_accuracy: 0.9073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc28c8687d0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# и обучаем\n",
    "model.fit(\n",
    "    np.array(data_X_train),\n",
    "    np.array(data_Y_train),\n",
    "    validation_data=(np.array(data_X_test), np.array(data_Y_test)), \n",
    "    epochs=1, \n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка обученности модели остается за вами. Этот пример лишь для того, чтобы показать как собрать сетку и скормить ей данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 5. Рекуррентая сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее рассмотрим более приближенную к SOTA модель. Ей является рекуррентая сеть, которая принимает эмбеддинги слов в предложении и генерирует для них распределение вероятностей. Основным отличием от прошлой в том, что теперь мы будем использовать соседние слова как раз за счет рекуррентого слоя. Для этой модели мы уже будем использовать функциональный способ задания модели все того же кераса (https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, TimeDistributed,Bidirectional, Input, Masking\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 50)          4944100   \n",
      "_________________________________________________________________\n",
      "blstm (Bidirectional)        (None, None, 200)         120800    \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, None, 17)          3417      \n",
      "=================================================================\n",
      "Total params: 5,068,317\n",
      "Trainable params: 5,068,317\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# В начале задается входной слой, в котором мы укажем входную размерность. \n",
    "# Это будет None, т.к. мы заранее не знаем, какой будет длина каждого предложения \n",
    "input_layer = Input(shape=(None,), name='input')\n",
    "# Далее идет все тот же слой эмеддинга, которому мы на вход подаем предыдущий слой (в этом и суть functional APO)\n",
    "embeddings_layer = Embedding(input_dim=len(word_to_index), output_dim=50, \n",
    "                             embeddings_initializer='random_uniform',\n",
    "                             name='embedding',\n",
    "                             mask_zero=True,\n",
    "                            )(input_layer)\n",
    "\n",
    "# Итак, основным слоем здесь будет двусторонний LSTM, который будет возвращать вектор для каждого слова (return_sequences=True) \n",
    "blstm_layer = Bidirectional(LSTM(100, return_sequences=True), name='blstm')(embeddings_layer)\n",
    "# Аналогично т.к. у нас здесь вектора для каждого слоя, то и полносвязный слой должен применяться для каждого слоя \n",
    "# по-отдельности. Поэтому полносвязный слой оборачивается в  TimeDistributed\n",
    "result_layer = TimeDistributed(Dense(len(pos_to_index), activation='softmax', name='result'))(blstm_layer)\n",
    "# собственно определяем модель входными и выходными слоями\n",
    "model = Model(inputs=[input_layer], outputs=result_layer)\n",
    "# компилируем\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# выводим архитектуру\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].set_weights([embs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам нужно было бы распределить слова по предложениям, распределить по группам по длине, выравнить предложения по длине в одной групе, заполнив недостающие слова паддингами. Но это довольно неприятный процесс, а мне просто хочется запустить сетку и проверить, что она вообще работает, что сошлись все разверности. Поэтому просто раскидаем по 10 слов с помощью `numpy.reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_idx = word_to_index['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "ys = []\n",
    "cur_sent = []\n",
    "cur_ys = []\n",
    "for x, y in zip(data_X, data_Y):\n",
    "    if x == dot_idx:\n",
    "        sentences.append(np.array(cur_sent))\n",
    "        ys.append(np.array(cur_ys).reshape(-1, 1))\n",
    "        cur_sent = []\n",
    "        cur_ys = []\n",
    "    else:\n",
    "        cur_sent.append(x)\n",
    "        cur_ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46100"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, sentences_test, ys_train, ys_test = train_test_split(sentences, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batched_data(sentences, ys, batch_size=10):\n",
    "    num_batches = len(sentences) // batch_size\n",
    "    result_X = []\n",
    "    result_Y = []\n",
    "    for i in range(num_batches):\n",
    "        x_batch = pad_sequences(sentences[i * batch_size : (i + 1) * batch_size])\n",
    "        y_batch  = pad_sequences(ys[i * batch_size : (i + 1) * batch_size])\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3457/3457 [==============================] - 182s 53ms/step - loss: 0.1754 - accuracy: 0.1214 - val_loss: 0.0874 - val_accuracy: 0.1200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc28fa8a850>"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    get_batched_data(sentences_train, ys_train), \n",
    "    validation_data=get_batched_data(sentences_test, ys_test),\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Плохая точность, потому что я так и не понял, как заставить керас игнорировать паддинги при подсчёте метрик кроме как писать кастомную метрику, так что доверимся лоссу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 6. Задание\n",
    "В качестве упражения предлагается довести до ума обучения второй модели: распределить слова по предложениям, написать тестирование модели и собственно посмотреть как оно обучилось. Тестировать предлагаю на последней 1000 предложений, обучать - на остальном. Кто уверен в своих желаниях, то может решить оригинальную задачу: предсказывать также грамматические категории. А мы же перейдем на следующем семинаре к более приятному фреймворку - PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
