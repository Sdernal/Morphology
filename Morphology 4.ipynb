{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфология 4\n",
    "В этом ноутбуке описана подготовка данных для задачи POS-tagging. А также пара простых моделей на keras, решающих данную задачу. Оригинальная задача и ноутбук есть в контесте: https://www.kaggle.com/c/rupos2018/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Загрузка корпуса\n",
    "Здесь мы прочитаем корпуса из csv и разложим их по спискам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для совместимости со вторым питоном\n",
    "from __future__ import print_function\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "ad156ee50218601b689e546907d0ce3109e155bd"
   },
   "outputs": [],
   "source": [
    "# Имена файлов с данными.\n",
    "TRAIN_FILENAME = \"data/input/train.csv\"\n",
    "TEST_FILENAME = \"data/input/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "fdbea001cf6b2aa7b763b4836050dc4fdabb6457"
   },
   "outputs": [],
   "source": [
    "# Считывание файлов.\n",
    "from collections import namedtuple\n",
    "WordForm = namedtuple(\"WordForm\", \"word pos gram\")\n",
    "\n",
    "def get_sentences(filename, is_train):\n",
    "    sentences = []\n",
    "    with io.open(filename, \"r\", encoding='utf-8') as r:\n",
    "        # Пропускаем заголовок\n",
    "        next(r)\n",
    "        sentence = [] # будем заполнять список предложений\n",
    "        for line in r:\n",
    "            # предложения отделены по '\\n'\n",
    "            if len(line.strip()) == 0:\n",
    "                if len(sentence) == 0:\n",
    "                    continue\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "                continue\n",
    "            if is_train:\n",
    "                # Формат: индекс\\tномер_в_предложении\\tсловоформа\\tPOS#Грамемы\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                pos = line.strip().split(\"\\t\")[3].split(\"#\")[0]\n",
    "                gram = line.strip().split(\"\\t\")[3].split(\"#\")[1]\n",
    "                sentence.append(WordForm(word, pos, gram))\n",
    "            else:\n",
    "                word = line.strip().split(\"\\t\")[2]\n",
    "                sentence.append(word)\n",
    "        if len(sentence) != 0:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "fe4e1691bc5ef95abc711c5f403b27b897647878",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = get_sentences(TRAIN_FILENAME, True)\n",
    "test = get_sentences(TEST_FILENAME, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А \t CONJ \t _\n",
      "ведь \t PART \t _\n",
      "для \t ADP \t _\n",
      "конкретных \t ADJ \t Case=Gen|Degree=Pos|Number=Plur\n",
      "изделий \t NOUN \t Animacy=Inan|Case=Gen|Gender=Neut|Number=Plur\n",
      "зачастую \t ADV \t Degree=Pos\n",
      "нужен \t ADJ \t Degree=Pos|Gender=Masc|Number=Sing|Variant=Brev\n",
      "монокристалл \t NOUN \t Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "не \t PART \t _\n",
      "только \t PART \t _\n"
     ]
    }
   ],
   "source": [
    "# Выыедем, что получилось\n",
    "for wordform in train[0][:10]:\n",
    "    print(wordform.word, '\\t', wordform.pos, '\\t', wordform.gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты далее будем использовать токены слов и POS-теги. Но чтобы определять грамматические значения нужно еще провести некоторые манипуляции с данными, описанные в оригинальном ноутубке. Мы же ограничимся только определением частей речи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Подготовка эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно в качестве признаков для обучения сеток используются словные эмбеддинги. Для этого можно скачать предобученные и сохранить их в матрицу, где в расположатся векторы эмбеддингах по индексам, соответсвующих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#запомним все уникальные слова и POS-теги в корпусе\n",
    "word_set = set()\n",
    "pos_set = set()\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        word_set.add(wordform.word.lower())\n",
    "        pos_set.add(wordform.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "лордом, барсук, бракосочетании, измерили, неутешительный, 157, снабжает, госструктур, метаболизме, сваливались, \n",
      "{'ADP', 'SCONJ', 'ADJ', 'CONJ', 'AUX', 'INTJ', 'PART', 'ADV', 'SYM', 'VERB', 'NUM', 'X', 'DET', 'PRON', 'NOUN', 'PUNCT', 'PROPN'}\n"
     ]
    }
   ],
   "source": [
    "for word in list(word_set)[:10]: \n",
    "    print(word, end=', ')\n",
    "print()\n",
    "print(pos_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Загрузите эмбеддинги c https://nlp.stanford.edu/projects/glove/ или другие, которые вам нравятся и пропишите путь к ним\n",
    "\n",
    "# https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
    "import numpy as np\n",
    "\n",
    "word_embeddings_path = 'data/ruwiki_20180420_100d.txt'\n",
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "embedding_size = None\n",
    "#Загружаем эмбеддинги\n",
    "with io.open(word_embeddings_path, 'r', encoding=\"utf-8\") as f_em:\n",
    "    for line in f_em:\n",
    "        split = line.strip().split(\" \")\n",
    "        # Совсем короткие строки пропускаем\n",
    "        if len(split) <= 2:\n",
    "            continue\n",
    "        # Встретив первую подходящую строку, фиксируем размер эмбеддингов\n",
    "        if embedding_size is None:\n",
    "            embedding_size = len(split) - 1\n",
    "            # Также нициализируем эмбеддинги для паддингов и неизвестных слов\n",
    "            word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "            word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "        # После этого все эмбеддинги должны быть одинаковой длины\n",
    "        if len(split) - 1 != embedding_size:\n",
    "            continue\n",
    "            \n",
    "        #Если слова нет в корпусе, то не будем для него запоминать эмбеддинг        \n",
    "        if (split[0] not in word_set):\n",
    "            continue\n",
    "        \n",
    "        word_embeddings.append(np.asarray(split[1:], dtype='float32'))\n",
    "        word2idx[split[0]] = len(word2idx)\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91330"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set & set(word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98880"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_set = set(word2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_set:\n",
    "    if not word in word2idx_set:\n",
    "        word2idx[word] = len(word2idx)\n",
    "        word2idx_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98880"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set & set(word2idx.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то эмбеддинги не сильно подходят для данного корпуса поэтому, просто инициализируем рандмно матрицу эмбеддингов при определении сетки. Вам же предлагается все-таки поискать подходящие эмбеддинги и использовать их при обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Подготовка данных\n",
    "Теперь нам остается только пронумеровать все слова и POS-теги и можно переходить к обучению сеток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {'PAD' : 0, 'UNK' : 1}\n",
    "for word in word_set:\n",
    "    word_to_index[word] = len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_to_index = {\"PAD\": 0}\n",
    "index_to_pos = {0: \"PAD\"}\n",
    "for pos in pos_set:\n",
    "    pos_to_index[pos] = len(pos_to_index)\n",
    "    index_to_pos[len(index_to_pos)] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для полносвязной сетки просто захреначим все в один список\n",
    "data_X = []\n",
    "data_Y = []\n",
    "for sent in train:\n",
    "    for wordform in sent:\n",
    "        data_X.append(word_to_index[wordform.word.lower()])\n",
    "        data_Y.append(pos_to_index[wordform.pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30242, 80873, 13379, 60856, 61168, 54174, 48329, 57079, 44875, 32592]\n",
      "[4, 7, 1, 3, 15, 8, 3, 15, 7, 7]\n"
     ]
    }
   ],
   "source": [
    "print(data_X[:10])\n",
    "print(data_Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 4. Полносвязная сеть\n",
    "Самой простой моделью является обычный перцептрон. На вход сетки будем подавать просто эмдеддинг каждого слова, на выходе ожидать распредедение вероятностей по тегам. В качестве фреймворка достаточно будет использовать keras и его Sequential модель (https://keras.io/models/sequential/), в которую слои добавляются последовательно, с помощью метода `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 50)             4944100   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                1818      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 18)                0         \n",
      "=================================================================\n",
      "Total params: 4,951,018\n",
      "Trainable params: 6,918\n",
      "Non-trainable params: 4,944,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# на самом деле на вход сетки будет добавляться индекс слова, а слой эмбеддинга будет возвращать для него вектор\n",
    "model.add(Embedding(input_length=1, input_dim=len(word_to_index), output_dim=50, embeddings_initializer='random_uniform',\n",
    "                    trainable=False)) # матрицу эмбеддингов просто инициализируем нормальным распределением и отключим обучение\n",
    "# далее нам нужно схлопнуть трехмерный тензор с одной фиктивной размерностью в двумерный\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100)) # основной полносвязный слой\n",
    "model.add(Activation('relu')) # для приличия добавим функцию активации\n",
    "model.add(Dense(len(pos_to_index))) # выходной слой тоже полносвязный размерности по кол-ву тегов\n",
    "model.add(Activation('softmax')) # ну и в конце делаем softmax, чтобы получить распределение\n",
    "model.summary() # вывод получившейся модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# компилируем модель\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "850689/850689 [==============================] - 4s 4us/step - loss: 1.4388 - accuracy: 0.5395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1539c7e48>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# и обучаем\n",
    "model.fit(np.array(data_X), np.array(data_Y), epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка обученности модели остается за вами. Этот пример лишь для того, чтобы показать как собрать сетку и скормить ей данные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 5. Рекуррентая сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее рассмотрим более приближенную к SOTA модель. Ей является рекуррентая сеть, которая принимает эмбеддинги слов в предложении и генерирует для них распределение вероятностей. Основным отличием от прошлой в том, что теперь мы будем использовать соседние слова как раз за счет рекуррентого слоя. Для этой модели мы уже будем использовать функциональный способ задания модели все того же кераса (https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.14992113, -0.1772393 ,  0.10720362, ..., -0.1589421 ,\n",
       "        -0.24490474, -0.02399684],\n",
       "       [ 0.1216    , -0.3251    , -0.1264    , ...,  0.2268    ,\n",
       "        -0.0277    , -0.0951    ],\n",
       "       ...,\n",
       "       [ 0.2397    , -0.0969    , -0.1736    , ...,  0.2676    ,\n",
       "        -0.1899    , -0.4815    ],\n",
       "       [ 0.301     ,  0.0516    , -0.0025    , ...,  0.1854    ,\n",
       "        -0.04      , -0.003     ],\n",
       "       [ 0.6093    ,  0.2419    ,  0.0996    , ...,  0.4635    ,\n",
       "        -0.1319    , -0.3728    ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_set) + 2, 100))\n",
    "for word, i in word2idx.items():\n",
    "    if i < len(word_embeddings):\n",
    "        embedding_matrix[i] = word_embeddings[i]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.uniform(size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, TimeDistributed,Bidirectional, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 100)         9888200   \n",
      "_________________________________________________________________\n",
      "blstm (Bidirectional)        (None, None, 400)         481600    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 18)          7218      \n",
      "=================================================================\n",
      "Total params: 10,377,018\n",
      "Trainable params: 488,818\n",
      "Non-trainable params: 9,888,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# В начале задается входной слой, в котором мы укажем входную размерность. \n",
    "# Это будет None, т.к. мы заранее не знаем, какой будет длина каждого предложения \n",
    "input_layer = Input(shape=(None,), name='input')\n",
    "# Далее идет все тот же слой эмеддинга, которому мы на вход подаем предыдущий слой (в этом и суть functional APO)\n",
    "embeddings_layer = Embedding(input_dim=len(word_to_index), output_dim=100, \n",
    "                             trainable=False, weights=[embedding_matrix],\n",
    "                             name='embedding')(input_layer)\n",
    "# Итак, основным слоем здесь будет двусторонний LSTM, который будет возвращать вектор для каждого слова (return_sequences=True) \n",
    "blstm_layer = Bidirectional(LSTM(200, return_sequences=True), name='blstm')(embeddings_layer)\n",
    "# Аналогично т.к. у нас здесь вектора для каждого слоя, то и полносвязный слой должен применяться для каждого слоя \n",
    "# по-отдельности. Поэтому полносвязный слой оборачивается в  TimeDistributed\n",
    "result_layer = TimeDistributed(Dense(len(pos_to_index), activation='softmax', name='result'))(blstm_layer)\n",
    "# собственно определяем модель входными и выходными слоями\n",
    "model = Model(inputs=[input_layer], outputs=result_layer)\n",
    "# компилируем\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# выводим архитектуру\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам нужно было бы распределить слова по предложениям, распределить по группам по длине, выравнить предложения по длине в одной групе, заполнив недостающие слова паддингами. Но это довольно неприятный процесс, а мне просто хочется запустить сетку и проверить, что она вообще работает, что сошлись все разверности. Поэтому просто раскидаем по 10 слов с помощью `numpy.reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnnX = np.reshape(data_X[:850000], (-1,10))\n",
    "# rnnY = np.reshape(data_Y[:850000], (-1,10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(rnnX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и проверим, что оно вообще работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(rnnX, rnnY, epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 6. Задание\n",
    "В качестве упражения предлагается довести до ума обучения второй модели: распределить слова по предложениям, написать тестирование модели и собственно посмотреть как оно обучилось. Тестировать предлагаю на последней 1000 предложений, обучать - на остальном. Кто уверен в своих желаниях, то может решить оригинальную задачу: предсказывать также грамматические категории. А мы же перейдем на следующем семинаре к более приятному фреймворку - PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_X = []\n",
    "# data_Y = []\n",
    "# for sent in train:\n",
    "#     data_X.append([word_to_index[wordform.word.lower()] for wordform in sent])\n",
    "#     data_Y.append([pos_to_index[wordform.pos] for wordform in sent])\n",
    "data_X = []\n",
    "data_Y = []\n",
    "for sent in train:\n",
    "    data_X.append([word2idx[wordform.word.lower()] for wordform in sent])\n",
    "    data_Y.append([pos_to_index[wordform.pos] for wordform in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size = 1000)\n",
    "\n",
    "# X_train, y_train = zip(*sorted(zip(X_train, y_train), key=lambda item: len(item[0])))\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_with_alignment(X, y):\n",
    "    batch_size = len(X)\n",
    "    max_sent_len = len(max(X, key=lambda item: len(item)))\n",
    "    X_batch = np.array([sent + [word2idx[\"PADDING_TOKEN\"]] * (max_sent_len - len(sent)) for sent in X])\n",
    "    pos_tags = np.array([sent + [pos_to_index[\"PAD\"]] * (max_sent_len - len(sent)) for sent in y])\n",
    "    \n",
    "    y_batch = np.zeros((len(X), max_sent_len, len(pos_to_index)))\n",
    "    for i, row in enumerate(pos_tags):\n",
    "        for j, pos in enumerate(row):\n",
    "            y_batch[i, j, pos] = 1\n",
    "    return (X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batches = []\n",
    "idx = 0\n",
    "while idx + batch_size < len(X_train):\n",
    "    idxs = np.arange(idx, idx + batch_size)\n",
    "    idx += batch_size\n",
    "    np.random.shuffle(idxs)\n",
    "    X = X_train[idxs]\n",
    "    y = y_train[idxs]\n",
    "    \n",
    "    batches.append(get_batch_with_alignment(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368/368 [02:18<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "for batch in tqdm.tqdm(batches):\n",
    "    model.train_on_batch(batch[0], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "loss 0.09356270033121108\n",
      "accuracy 0.9724166393280029\n"
     ]
    }
   ],
   "source": [
    "batch = get_batch_with_alignment(X_test, y_test)\n",
    "res = model.evaluate(batch[0], batch[1])\n",
    "\n",
    "for name, val in zip(model.metrics_names, res):\n",
    "    print(name, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
