{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wsl wget 'https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-train.conllu'\n",
    "!wsl mv ru_syntagrus-ud-train.conllu data/ru_syntagrus-ud-train.conllu\n",
    "!wsl wget 'https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-dev.conllu'\n",
    "!wsl mv ru_syntagrus-ud-dev.conllu data/ru_syntagrus-ud-dev.conllu\n",
    "!wsl wget 'https://github.com/UniversalDependencies/UD_Russian-SynTagRus/raw/master/ru_syntagrus-ud-test.conllu'\n",
    "!wsl mv ru_syntagrus-ud-test.conllu data/ru_syntagrus-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wsl wget 'https://s3.amazonaws.com/realworldnlpbook/data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-train.conllu'\n",
    "# !wsl mv en_ewt-ud-train.conllu data/en_ewt-ud-train.conllu\n",
    "# !wsl wget 'https://s3.amazonaws.com/realworldnlpbook/data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-dev.conllu'\n",
    "# !wsl mv en_ewt-ud-dev.conllu data/en_ewt-ud-dev.conllu\n",
    "# !wsl wget 'https://s3.amazonaws.com/realworldnlpbook/data/ud-treebanks-v2.3/UD_English-EWT/en_ewt-ud-test.conllu'\n",
    "# !wsl mv en_ewt-ud-test.conllu data/en_ewt-ud-test.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20000\n",
      "22000\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "# for file in [\"train\", \"dev\", \"test\"]:\n",
    "file = 'train'\n",
    "word2lemma = dict()\n",
    "with open(f'data/ru_syntagrus-ud-{file}.conllu') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "for line in data:\n",
    "    if line[0] == '#':\n",
    "        continue\n",
    "    if line[0] == '\\n':\n",
    "#             max_len = max(max_len, max(len(left), len(right)))\n",
    "#             if len(left) < 100:\n",
    "#                 left_sent = \" \".join(left).replace(\"\\'\", \" \").replace(\"\\\"\", \" \")\n",
    "#                 right_sent = \" \".join(right)\n",
    "#                 outlines.append(f\"{left_sent}\\t{right_sent}\\n\")\n",
    "#             left, right = [], []\n",
    "        continue\n",
    "    else:\n",
    "        arr = line.split('\\t')\n",
    "        if len(arr[1].strip()) < 5 or len(arr[1]) > 20:\n",
    "            continue\n",
    "        if arr[1] not in word2lemma:\n",
    "            max_len=max(max_len, len(arr[1]))\n",
    "            word2lemma[arr[1]] = arr[2]\n",
    "\n",
    "\n",
    "word_list = list(word2lemma.items())\n",
    "startes = {\n",
    "    'train' : 0,\n",
    "    'dev' : 20000,\n",
    "    'test' : 22000\n",
    "}\n",
    "random.shuffle(word_list)\n",
    "for file in [\"train\", \"dev\", \"test\"]:\n",
    "    outlines = []\n",
    "    if file == \"train\":\n",
    "        limit = 20000\n",
    "    else:\n",
    "        limit = 2000\n",
    "    start = startes[file]\n",
    "    print(start)\n",
    "    for key, value in word_list[start:start+limit]:\n",
    "        outlines.append(f\"{key}\\t{value}\\n\")\n",
    "    with open(f'data/{file}.txt', 'w') as f:\n",
    "        f.writelines(outlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!allennlp train -s log_300 simple_tagger_pos.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models.archival import load_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mySeq2Seq import Seq2SeqEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "_jsonnet not loaded, treating C:\\Users\\allok\\AppData\\Local\\Temp\\tmpyxw_wlz2\\config.json as json\n",
      "C:\\Users\\allok\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "mymodel = load_archive('log_300/model.tar.gz', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSeq2Seq(\n",
       "  (_source_embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (_encoder): PytorchSeq2SeqWrapper(\n",
       "    (_module): LSTM(20, 100, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (_attention): LinearAttention()\n",
       "  (_target_embedder): Embedding()\n",
       "  (_decoder_cell): LSTMCell(220, 200)\n",
       "  (_output_projection_layer): Linear(in_features=200, out_features=143, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reader = mymodel.config[\"dataset_reader\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(<allennlp.common.params.Params object at 0x000002854CF1CDD8>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_reader.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_reader_params = mymodel.config[\"dataset_reader\"]\n",
    "dataset_reader = DatasetReader.from_params(dataset_reader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 20619.14it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = dataset_reader.read('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:45<00:00, 44.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for i in tqdm(test_data):\n",
    "    gold = (''.join([ str(t) for t in i[\"target_tokens\"][1:-1]]))\n",
    "    pred = ''.join([str(t) for t in mymodel.model.forward_on_instance(i)[\"predicted_tokens\"]])\n",
    "    if gold == pred:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(float(correct) / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'отрицать'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([str(t) for t in mymodel.model.forward_on_instance(dataset_reader.text_to_instance(\"отрицала\"))[\"predicted_tokens\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "ex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
