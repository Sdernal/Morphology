{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.seq2seq import Seq2SeqDatasetReader\n",
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers.character_tokenizer import CharacterTokenizer\n",
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.nn.activations import Activation\n",
    "from allennlp.models.encoder_decoders.simple_seq2seq import SimpleSeq2Seq\n",
    "from allennlp.modules.attention import LinearAttention, BilinearAttention, DotProductAttention\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, StackedSelfAttentionEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.predictors import SimpleSeq2SeqPredictor\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "from allennlp.data import DatasetReader\n",
    "from allennlp.models.archival import load_archive\n",
    "\n",
    "from allennlp.models.archival import archive_model\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:00, 24330.91it/s]\n",
      "2000it [00:00, 48780.62it/s]\n",
      "2000it [00:00, 9523.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 22000/22000 [00:00<00:00, 99999.99it/s]\n"
     ]
    }
   ],
   "source": [
    "CUDA_DEVICE = 0\n",
    "\n",
    "reader = Seq2SeqDatasetReader(\n",
    "    source_tokenizer=CharacterTokenizer(),\n",
    ")\n",
    "\n",
    "train_dataset = reader.read('data/train.txt')\n",
    "validation_dataset = reader.read('data/dev.txt')\n",
    "test_dataset = reader.read('data/test.txt')\n",
    "\n",
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), embedding_dim=20)\n",
    "encoder = PytorchSeq2SeqWrapper(torch.nn.LSTM(20, 100, batch_first=True, bidirectional=True, dropout=0.3))\n",
    "\n",
    "source_embedder = BasicTextFieldEmbedder({\"tokens\": embedding})\n",
    "\n",
    "attention = LinearAttention(200, 200)\n",
    "\n",
    "model = SimpleSeq2Seq(vocab, source_embedder, encoder, 21,\n",
    "                      attention=attention,\n",
    "                      beam_size=5,\n",
    "                      scheduled_sampling_ratio=0.5,\n",
    "                      use_bleu=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "iterator = BasicIterator(batch_size=4096)\n",
    "\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSeq2Seq(\n",
       "  (_source_embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (_encoder): PytorchSeq2SeqWrapper(\n",
       "    (_module): LSTM(20, 100, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "  )\n",
       "  (_attention): LinearAttention()\n",
       "  (_target_embedder): Embedding()\n",
       "  (_decoder_cell): LSTMCell(220, 200)\n",
       "  (_output_projection_layer): Linear(in_features=200, out_features=141, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device=CUDA_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:allennlp.training.trainer:You provided a validation dataset but patience was set to None, meaning that early stopping is disabled\n",
      "loss: 3.9660 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.03it/s]\n",
      "BLEU: 0.0000, loss: 3.2764 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.05s/it]\n",
      "loss: 3.2246 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.93it/s]\n",
      "BLEU: 0.0136, loss: 3.1169 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.39s/it]\n",
      "loss: 3.1057 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.94it/s]\n",
      "BLEU: 0.0000, loss: 3.0275 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.71s/it]\n",
      "loss: 3.0314 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.63it/s]\n",
      "BLEU: 0.0000, loss: 3.0279 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.36s/it]\n",
      "loss: 2.9909 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.94it/s]\n",
      "BLEU: 0.0000, loss: 2.9383 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.27s/it]\n",
      "loss: 2.9183 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.96it/s]\n",
      "BLEU: 0.0000, loss: 3.0448 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.96s/it]\n",
      "loss: 2.9506 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.67it/s]\n",
      "BLEU: 0.0000, loss: 2.8792 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.95s/it]\n",
      "loss: 2.8844 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.97it/s]\n",
      "BLEU: 0.0071, loss: 2.8696 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.55s/it]\n",
      "loss: 2.8432 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.94it/s]\n",
      "BLEU: 0.0000, loss: 2.8171 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.70s/it]\n",
      "loss: 2.7985 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.64it/s]\n",
      "BLEU: 0.0155, loss: 2.7772 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.25s/it]\n",
      "loss: 2.7740 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.91it/s]\n",
      "BLEU: 0.0129, loss: 2.7569 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.71s/it]\n",
      "loss: 2.7673 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.92it/s]\n",
      "BLEU: 0.0135, loss: 2.7309 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.17s/it]\n",
      "loss: 2.7274 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.64it/s]\n",
      "BLEU: 0.0233, loss: 2.6386 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.65s/it]\n",
      "loss: 2.6824 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.94it/s]\n",
      "BLEU: 0.0219, loss: 2.5656 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.55s/it]\n",
      "loss: 2.7026 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.90it/s]\n",
      "BLEU: 0.0308, loss: 2.5389 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.66s/it]\n",
      "loss: 2.6362 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.65it/s]\n",
      "BLEU: 0.0300, loss: 2.5182 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.62s/it]\n",
      "loss: 2.6395 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.93it/s]\n",
      "BLEU: 0.0301, loss: 2.4937 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.84s/it]\n",
      "loss: 2.5578 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.91it/s]\n",
      "BLEU: 0.0341, loss: 2.4573 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.94s/it]\n",
      "loss: 2.5501 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.60it/s]\n",
      "BLEU: 0.0439, loss: 2.5120 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.01s/it]\n",
      "loss: 2.5785 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.90it/s]\n",
      "BLEU: 0.0321, loss: 2.4240 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.53s/it]\n",
      "loss: 2.5795 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.90it/s]\n",
      "BLEU: 0.0269, loss: 2.4358 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.45s/it]\n",
      "loss: 2.5571 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.93it/s]\n",
      "BLEU: 0.0417, loss: 2.4072 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.73s/it]\n",
      "loss: 2.5748 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.65it/s]\n",
      "BLEU: 0.0454, loss: 2.4231 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.83s/it]\n",
      "loss: 2.5331 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.91it/s]\n",
      "BLEU: 0.0358, loss: 2.3611 ||: 100%|█████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.82s/it]\n",
      "loss: 2.4533 ||: 100%|███████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.90it/s]\n",
      "  0%|                                                                                            | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-34381fb0cdbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m                   cuda_device=CUDA_DEVICE)\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\allennlp\\training\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m                     \u001b[1;31m# We have a validation set, so compute all the metrics on it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m                     \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validation_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m                     \u001b[0mval_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\allennlp\\training\\trainer.py\u001b[0m in \u001b[0;36m_validation_loss\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_group\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_generator_tqdm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfor_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;31m# You shouldn't necessarily have to compute a loss for validation, so we allow for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\allennlp\\training\\trainer.py\u001b[0m in \u001b[0;36mbatch_loss\u001b[1;34m(self, batch_group, for_training)\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_devices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m             \u001b[0moutput_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\allennlp\\models\\encoder_decoders\\simple_seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, source_tokens, target_tokens)\u001b[0m\n\u001b[0;32m    229\u001b[0m                 \u001b[1;31m# shape: (batch_size, max_predicted_sequence_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[0mbest_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_k_predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bleu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tokens\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\allennlp\\training\\metrics\\bleu.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, predictions, gold_targets)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mngram_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ngram_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             precision_matches, precision_totals = self._get_modified_precision_counts(\n\u001b[1;32m--> 135\u001b[1;33m                     predictions, gold_targets, ngram_size)\n\u001b[0m\u001b[0;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_precision_matches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprecision_matches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_precision_totals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mprecision_totals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\allennlp\\training\\metrics\\bleu.py\u001b[0m in \u001b[0;36m_get_modified_precision_counts\u001b[1;34m(self, predicted_tokens, reference_tokens, ngram_size)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mpredicted_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mreference_row\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreference_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mpredicted_ngram_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[0mreference_ngram_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreference_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mngram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredicted_ngram_counts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\allennlp\\training\\metrics\\bleu.py\u001b[0m in \u001b[0;36m_ngrams\u001b[1;34m(self, tensor, ngram_size)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mngram_counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstart_position\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtensor_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_position\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtensor_slice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mngram_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, split_size, dim)\u001b[0m\n\u001b[0;32m    373\u001b[0m         \"\"\"\n\u001b[0;32m    374\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_with_sizes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  num_epochs=100,\n",
    "                  model_save_interval=10,\n",
    "                  num_serialized_models_to_keep=3,\n",
    "                  serialization_dir='log',\n",
    "                  cuda_device=CUDA_DEVICE)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp simple_seq2seq.json log/config.json\n",
    "vocab.save_to_files(os.path.join(\"log\", \"vocabulary\"))\n",
    "archive_model(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = load_archive('log/model.tar.gz', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 6711.44it/s]\n"
     ]
    }
   ],
   "source": [
    "mymodel.model.eval()\n",
    "dataset_reader_params = mymodel.config[\"dataset_reader\"]\n",
    "dataset_reader = DatasetReader.from_params(dataset_reader_params)\n",
    "test_data = dataset_reader.read('data/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:46<00:00, 42.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for i in tqdm(test_data):\n",
    "    gold = (''.join([ str(t) for t in i[\"target_tokens\"][1:-1]]))\n",
    "    pred = ''.join([str(t) for t in mymodel.model.forward_on_instance(i)[\"predicted_tokens\"]])\n",
    "    if gold == pred:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(float(correct) / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_log_probabilities': array([-0.27276325, -2.9633589 , -3.3173828 , -3.5159779 , -4.247253  ],\n",
       "       dtype=float32),\n",
       " 'predictions': array([[18, 21, 17,  5,  8, 16,  3],\n",
       "        [18, 21, 17,  5,  8,  3,  3],\n",
       "        [18, 21, 17,  5,  8,  5,  3],\n",
       "        [18, 21, 17,  5, 13, 16,  3],\n",
       "        [18, 21, 17,  5, 13,  3,  3]], dtype=int64),\n",
       " 'predicted_tokens': ['д', 'у', 'м', 'а', 'т', 'ь']}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.model.forward_on_instance(dataset_reader.text_to_instance(\"думала\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
