{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-nmt\n",
      "Cloning into '../bert-nmt'...\n",
      "remote: Enumerating objects: 290, done.\u001b[K\n",
      "remote: Counting objects: 100% (290/290), done.\u001b[K\n",
      "remote: Compressing objects: 100% (239/239), done.\u001b[K\n",
      "remote: Total 290 (delta 53), reused 262 (delta 29), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (290/290), 2.58 MiB | 851.00 KiB/s, done.\n",
      "Resolving deltas: 100% (53/53), done.\n",
      "Loading subword\n",
      "Cloning into '../subword'...\n",
      "remote: Enumerating objects: 576, done.\u001b[K\n",
      "remote: Total 576 (delta 0), reused 0 (delta 0), pack-reused 576\u001b[K\n",
      "Receiving objects: 100% (576/576), 233.30 KiB | 940.00 KiB/s, done.\n",
      "Resolving deltas: 100% (349/349), done.\n",
      "Loading gec-pseudodata\n",
      "Cloning into '../gec-pseudodata'...\n",
      "remote: Enumerating objects: 104, done.\u001b[K\n",
      "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
      "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
      "remote: Total 104 (delta 52), reused 75 (delta 33), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (104/104), 557.73 KiB | 341.00 KiB/s, done.\n",
      "Resolving deltas: 100% (52/52), done.\n",
      "Loading pre-trained GEC model\n",
      "--2020-12-22 18:20:20--  https://gec-pseudo-data.s3-ap-northeast-1.amazonaws.com/ldc_giga.spell_error.pretrain.checkpoint_last.pt\n",
      "Resolving gec-pseudo-data.s3-ap-northeast-1.amazonaws.com (gec-pseudo-data.s3-ap-northeast-1.amazonaws.com)... 52.219.4.75\n",
      "Connecting to gec-pseudo-data.s3-ap-northeast-1.amazonaws.com (gec-pseudo-data.s3-ap-northeast-1.amazonaws.com)|52.219.4.75|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2017897726 (1,9G) [binary/octet-stream]\n",
      "Saving to: ‘../pseudo_model/ldc_giga.spell_error.pretrain.checkpoint_last.pt’\n",
      "\n",
      "ldc_giga.spell_erro 100%[===================>]   1,88G  2,89MB/s    in 13m 35s \n",
      "\n",
      "2020-12-22 18:33:56 (2,36 MB/s) - ‘../pseudo_model/ldc_giga.spell_error.pretrain.checkpoint_last.pt’ saved [2017897726/2017897726]\n",
      "\n",
      "Loading pre-trained BERT model\n",
      "--2020-12-22 18:33:56--  https://drive.google.com/uc?export=download&id=1wwrTLQ2cg8VYDqXezqZCm6ErJcPKppdm\n",
      "Resolving drive.google.com (drive.google.com)... 64.233.161.194, 2a00:1450:4010:c01::c2\n",
      "Connecting to drive.google.com (drive.google.com)|64.233.161.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-08-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/b4859vtti7ab49he1hh82cq110muqsup/1608651225000/00733782629194023927/*/1wwrTLQ2cg8VYDqXezqZCm6ErJcPKppdm?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-12-22 18:33:56--  https://doc-08-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/b4859vtti7ab49he1hh82cq110muqsup/1608651225000/00733782629194023927/*/1wwrTLQ2cg8VYDqXezqZCm6ErJcPKppdm?e=download\n",
      "Resolving doc-08-1k-docs.googleusercontent.com (doc-08-1k-docs.googleusercontent.com)... 64.233.161.132, 2a00:1450:4010:c01::84\n",
      "Connecting to doc-08-1k-docs.googleusercontent.com (doc-08-1k-docs.googleusercontent.com)|64.233.161.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 566 [application/json]\n",
      "Saving to: ‘../bert-base-cased/config.json’\n",
      "\n",
      "../bert-base-cased/ 100%[===================>]     566  --.-KB/s    in 0s      \n",
      "\n",
      "2020-12-22 18:33:57 (48,9 MB/s) - ‘../bert-base-cased/config.json’ saved [566/566]\n",
      "\n",
      "--2020-12-22 18:33:57--  https://drive.google.com/uc?export=download&id=1D2YcxaSO-NQN8-HaybGg16v3FELNxOI5\n",
      "Resolving drive.google.com (drive.google.com)... 64.233.161.194, 2a00:1450:4010:c01::c2\n",
      "Connecting to drive.google.com (drive.google.com)|64.233.161.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0c-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u6a5u4fdgnshr96b1ehi957rh4ujmhdv/1608651225000/00733782629194023927/*/1D2YcxaSO-NQN8-HaybGg16v3FELNxOI5?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2020-12-22 18:33:58--  https://doc-0c-1k-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/u6a5u4fdgnshr96b1ehi957rh4ujmhdv/1608651225000/00733782629194023927/*/1D2YcxaSO-NQN8-HaybGg16v3FELNxOI5?e=download\n",
      "Resolving doc-0c-1k-docs.googleusercontent.com (doc-0c-1k-docs.googleusercontent.com)... 64.233.161.132, 2a00:1450:4010:c01::84\n",
      "Connecting to doc-0c-1k-docs.googleusercontent.com (doc-0c-1k-docs.googleusercontent.com)|64.233.161.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 213450 (208K) [text/plain]\n",
      "Saving to: ‘../bert-base-cased/vocab.txt’\n",
      "\n",
      "../bert-base-cased/ 100%[===================>] 208,45K  --.-KB/s    in 0,06s   \n",
      "\n",
      "2020-12-22 18:33:58 (3,62 MB/s) - ‘../bert-base-cased/vocab.txt’ saved [213450/213450]\n",
      "\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   408    0   408    0     0   1505      0 --:--:-- --:--:-- --:--:--  1505\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0:-- --:--:-- --:--:--     0\n",
      "100  413M    0  413M    0     0  16.0M      0 --:--:--  0:00:25 --:--:-- 17.0M\n",
      "Loading wi+locness\n",
      "--2020-12-22 18:34:25--  https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz\n",
      "Resolving www.cl.cam.ac.uk (www.cl.cam.ac.uk)... 128.232.0.20, 2a05:b400:110::80:14\n",
      "Connecting to www.cl.cam.ac.uk (www.cl.cam.ac.uk)|128.232.0.20|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6120469 (5,8M) [application/x-gzip]\n",
      "Saving to: ‘../data/wi+locness_v2.1.bea19.tar.gz’\n",
      "\n",
      "wi+locness_v2.1.bea 100%[===================>]   5,84M  6,16MB/s    in 0,9s    \n",
      "\n",
      "2020-12-22 18:34:27 (6,16 MB/s) - ‘../data/wi+locness_v2.1.bea19.tar.gz’ saved [6120469/6120469]\n",
      "\n",
      "wi+locness/\n",
      "wi+locness/json_to_m2.py\n",
      "wi+locness/licence.wi.txt\n",
      "wi+locness/readme.txt\n",
      "wi+locness/license.locness.txt\n",
      "wi+locness/json/\n",
      "wi+locness/json/A.dev.json\n",
      "wi+locness/json/A.train.json\n",
      "wi+locness/json/B.dev.json\n",
      "wi+locness/json/B.train.json\n",
      "wi+locness/json/C.dev.json\n",
      "wi+locness/json/C.train.json\n",
      "wi+locness/json/N.dev.json\n",
      "wi+locness/m2/\n",
      "wi+locness/m2/ABCN.dev.gold.bea19.m2\n",
      "wi+locness/m2/A.train.gold.bea19.m2\n",
      "wi+locness/m2/A.dev.gold.bea19.m2\n",
      "wi+locness/m2/B.train.gold.bea19.m2\n",
      "wi+locness/m2/B.dev.gold.bea19.m2\n",
      "wi+locness/m2/C.train.gold.bea19.m2\n",
      "wi+locness/m2/C.dev.gold.bea19.m2\n",
      "wi+locness/m2/N.dev.gold.bea19.m2\n",
      "wi+locness/m2/ABC.train.gold.bea19.m2\n",
      "wi+locness/test/\n",
      "wi+locness/test/ABCN.test.bea19.orig\n",
      "wi+locness/test/readme.txt\n",
      "Loading pre-trained model\n",
      "--2020-12-22 18:34:28--  https://gec-pseudo-data.s3-ap-northeast-1.amazonaws.com/ldc_giga.spell_error.pretrain.checkpoint_last.pt\n",
      "Resolving gec-pseudo-data.s3-ap-northeast-1.amazonaws.com (gec-pseudo-data.s3-ap-northeast-1.amazonaws.com)... 52.219.8.3\n",
      "Connecting to gec-pseudo-data.s3-ap-northeast-1.amazonaws.com (gec-pseudo-data.s3-ap-northeast-1.amazonaws.com)|52.219.8.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2017897726 (1,9G) [binary/octet-stream]\n",
      "Saving to: ‘../pretrained/ldc_giga.spell_error.pretrain.checkpoint_last.pt’\n",
      "\n",
      "../pretrained/ldc_g 100%[===================>]   1,88G  4,15MB/s    in 10m 10s \n",
      "\n",
      "2020-12-22 18:44:39 (3,15 MB/s) - ‘../pretrained/ldc_giga.spell_error.pretrain.checkpoint_last.pt’ saved [2017897726/2017897726]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.3.0\n",
      "Uninstalling torch-1.3.0:\n",
      "  Successfully uninstalled torch-1.3.0\n",
      "Collecting torch==1.1.0\n",
      "  Downloading torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 676.9 MB 4.1 kB/s eta 0:00:013    |█▏                              | 25.1 MB 10.3 MB/s eta 0:01:04     |██▎                             | 47.3 MB 8.0 MB/s eta 0:01:20     |█████▏                          | 108.7 MB 10.5 MB/s eta 0:00:55     |█████▏                          | 109.6 MB 10.5 MB/s eta 0:00:55     |█████▍                          | 114.4 MB 9.5 MB/s eta 0:01:00     |█████▊                          | 120.2 MB 11.8 MB/s eta 0:00:48     |██████▏                         | 129.8 MB 8.9 MB/s eta 0:01:02     |██████▋                         | 140.8 MB 9.1 MB/s eta 0:00:59     |█████████▎                      | 195.5 MB 8.2 MB/s eta 0:00:59     |█████████▋                      | 204.4 MB 8.9 MB/s eta 0:00:54     |██████████                      | 212.8 MB 11.1 MB/s eta 0:00:42     |████████████▍                   | 261.7 MB 11.8 MB/s eta 0:00:36     |██████████████▌                 | 306.8 MB 8.8 MB/s eta 0:00:43     |█████████████████▋              | 372.6 MB 7.8 MB/s eta 0:00:40     |██████████████████▏             | 384.0 MB 12.0 MB/s eta 0:00:25     |██████████████████▎             | 387.7 MB 12.0 MB/s eta 0:00:25     |███████████████████▉            | 419.7 MB 8.5 MB/s eta 0:00:31     |█████████████████████▎          | 449.8 MB 10.6 MB/s eta 0:00:22     |██████████████████████          | 466.9 MB 7.1 MB/s eta 0:00:30     |███████████████████████▍        | 495.6 MB 11.1 MB/s eta 0:00:17     |███████████████████████▌        | 496.7 MB 11.1 MB/s eta 0:00:17     |███████████████████████▊        | 501.4 MB 1.7 MB/s eta 0:01:42     |███████████████████████▊        | 501.8 MB 1.7 MB/s eta 0:01:42     |████████████████████████▍       | 516.6 MB 9.4 MB/s eta 0:00:18     |████████████████████████▉       | 524.5 MB 7.0 MB/s eta 0:00:22     |█████████████████████████       | 526.6 MB 7.0 MB/s eta 0:00:22     |█████████████████████████       | 530.4 MB 10.1 MB/s eta 0:00:15     |█████████████████████████▎      | 533.7 MB 10.1 MB/s eta 0:00:15     |█████████████████████████▍      | 535.9 MB 10.1 MB/s eta 0:00:14     |██████████████████████████▏     | 554.2 MB 14.1 MB/s eta 0:00:09     |██████████████████████████▊     | 564.3 MB 7.4 MB/s eta 0:00:16     |███████████████████████████▌    | 580.6 MB 9.9 MB/s eta 0:00:10     |█████████████████████████████▋  | 626.8 MB 8.6 MB/s eta 0:00:06     |███████████████████████████████▋| 669.7 MB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/mlepekhin/anaconda3/envs/mlepekhin_py36/lib/python3.6/site-packages (from torch==1.1.0) (1.18.5)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch -y\n",
    "!pip install 'torch==1.1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process file already exists\n",
      "| distributed init (rank 3): tcp://localhost:11751\n",
      "| distributed init (rank 2): tcp://localhost:11751\n",
      "| distributed init (rank 1): tcp://localhost:11751\n",
      "| distributed init (rank 0): tcp://localhost:11751\n",
      "| initialized host seven as rank 3\n",
      "| initialized host seven as rank 2\n",
      "| initialized host seven as rank 1\n",
      "| initialized host seven as rank 0\n",
      "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_s2_vaswani_wmt_en_de_big', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='../bert-base-cased', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=1.0, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='../process/bin', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_input_dim=1024, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=1024, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:11751', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=4, dropout=0.3, encoder_attention_heads=16, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.3, encoder_bert_mixup=False, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format='simple', log_interval=1000, lr=[3e-05], lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.7, lr_threshold=0.0001, mask_cls_sep=False, max_epoch=5, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_update=0, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-06, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=True, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='../model/bert-base-cased/2222', save_interval=1, save_interval_updates=0, seed=2222, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='src', target_lang='trg', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=True, warmup_nmt_file='checkpoint_last.pt', weight_decay=0.0)\n",
      "| [src] dictionary: 8294 types\n",
      "| [trg] dictionary: 8269 types\n",
      "| ../process/bin valid src-trg 4384 examples\n",
      "bert_gates [True, True, True, True, True, True]\n",
      "TransformerS2Model(\n",
      "  (encoder): TransformerS2Encoder(\n",
      "    (embed_tokens): Embedding(8294, 1024, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): Embedding(8269, 1024, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm(torch.Size([1024]), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bert_encoder): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "| model transformer_s2_vaswani_wmt_en_de_big, criterion LabelSmoothedCrossEntropyCriterion\n",
      "| num. model params: 354184960 (num. trained: 245874688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| training on 4 GPUs\n",
      "| max tokens per GPU = 4096 and max sentences per GPU = None\n",
      "Model will load checkpoint from ../model/bert-base-cased/2222/checkpoint_last.pt\n",
      "| loaded checkpoint ../model/bert-base-cased/2222/checkpoint_last.pt (epoch 10 @ 0 updates)\n",
      "| NOTICE: your device may support faster training with --fp16\n",
      "| loading train data for epoch 0\n",
      "| ../process/bin train src-trg 34308 examples\n",
      "| saved checkpoint ../model/bert-base-cased/2222/checkpoint0.pt (epoch 0 @ 0 updates) (writing took 15.755367994308472 seconds)\n"
     ]
    }
   ],
   "source": [
    "!./train.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
