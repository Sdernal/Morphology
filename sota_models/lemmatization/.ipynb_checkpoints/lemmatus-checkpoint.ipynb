{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq approach to lemmatization task based on this paper - https://www.aclweb.org/anthology/N18-1126.pdf\n",
    "\n",
    "# Seq2Seq model is conditional GRU from scratch based on this paper https://www.aclweb.org/anthology/E17-3017.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "with open(\"data/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu\", 'r') as f:\n",
    "    data = f.read()\n",
    "sentences = conllu.parse(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "allowed_char_re = re.compile(r'[^a-zA-Zа-яА-Я.,?!ёЁ\"]')\n",
    "def preprocess(text):\n",
    "    text = allowed_char_re.sub(' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_skip_re = re.compile(r'[а-яА-ЯёЁ]+')\n",
    "def is_skip(text):\n",
    "    return not is_skip_re.fullmatch(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence_tokens):\n",
    "    text = ''\n",
    "    cur_pointer = 0\n",
    "    lemmas = []\n",
    "    for s in sentence_tokens:\n",
    "        if not is_skip(s['form']):\n",
    "            lemmas.append(\n",
    "                    {\n",
    "                        \"form\": s['form'],\n",
    "                        \"lemma\": s['lemma'],\n",
    "                        \"start_idx\": cur_pointer,\n",
    "                        \"end_idx\": cur_pointer + len(s['form'])\n",
    "                    }\n",
    "                )\n",
    "        if s['misc'] is not None and s['misc']['SpaceAfter'] == 'No':\n",
    "            cur_pointer += len(s['form'])\n",
    "            text += s['form']\n",
    "        else:\n",
    "            cur_pointer += len(s['form']) + 1\n",
    "            text += s['form'] + ' '\n",
    "\n",
    "    text = text.strip(' ')\n",
    "    return text, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples_from_converted(text, lemma, window_size):\n",
    "    \n",
    "    chars = list(text)\n",
    "    target_word = lemma['form']\n",
    "    \n",
    "    left_border = max(0, lemma['start_idx'] - window_size)\n",
    "    left_context = chars[left_border:lemma['start_idx']]\n",
    "    \n",
    "    right_border = min(len(text), lemma['end_idx'] + window_size)\n",
    "    right_context = chars[lemma['end_idx']:right_border]\n",
    "    \n",
    "    target =  ['<lc>'] + list(target_word) + ['<rc>']\n",
    "    return target, (left_context, right_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(sentences, window_size):\n",
    "    dataset = []\n",
    "    for sentence in sentences:\n",
    "        text, lemmas = prepare_sentence(sentence)\n",
    "        for lemma in lemmas:\n",
    "            target, contexts = get_examples_from_converted(text, lemma, window_size)\n",
    "            dataset.append({\n",
    "                \"input\": contexts[0] + target + contexts[1],\n",
    "                \"target\": list(lemma[\"lemma\"])\n",
    "            })\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as  nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizationDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, window_size):\n",
    "        char_dataset = get_dataset(sentences, window_size=window_size)\n",
    "        self.input_token2idx = self.create_token2idx(\"input\", char_dataset)\n",
    "        self.target_token2idx = self.create_token2idx(\"target\", char_dataset)\n",
    "        \n",
    "        self.dataset = self.tokenize_dataset(char_dataset)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_token2idx(dest, char_dataset):\n",
    "        chars = set.union(*map(lambda x: set(x[dest]), char_dataset))\n",
    "        if dest == \"input\":\n",
    "            token2idx = {char: idx for idx, char in enumerate(chars, start=1)}\n",
    "        elif dest == \"target\":\n",
    "            token2idx = {char: idx for idx, char in enumerate(chars, start=3)}\n",
    "            token2idx[\"<BOS>\"] = 1\n",
    "            token2idx[\"<EOS>\"] = 2\n",
    "        token2idx[\"<PAD>\"] = 0\n",
    "        return token2idx\n",
    "    \n",
    "    def tokenize_dataset(self, char_dataset):\n",
    "        result = []\n",
    "        for data_point in char_dataset:\n",
    "            \n",
    "            cur_inp = []\n",
    "            for char in data_point[\"input\"]:\n",
    "                cur_inp.append(self.input_token2idx[char])\n",
    "                \n",
    "            cur_targ = []\n",
    "            for char in data_point[\"target\"]:\n",
    "                cur_targ.append(self.target_token2idx[char])\n",
    "            cur_targ = [self.target_token2idx[\"<BOS>\"]] + cur_targ + [self.target_token2idx[\"<EOS>\"]]\n",
    "            \n",
    "                \n",
    "            result.append(\n",
    "                {\n",
    "                    \"input\": cur_inp,\n",
    "                    \"target\": cur_targ ,\n",
    "                }\n",
    "            )\n",
    "        return result\n",
    "            \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LemmatizationDataset(sentences, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for data_point in batch:\n",
    "        inputs.append(torch.LongTensor(data_point[\"input\"]))\n",
    "        targets.append(torch.LongTensor(data_point[\"target\"]))\n",
    "    \n",
    "    inp_tensor =  torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
    "    target_tensor =  torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return inp_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_data_loaders(dataset, batch_size):\n",
    "    train, valid = train_test_split(dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid,\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return {\n",
    "        \"train\": train_loader,\n",
    "        \"valid\": valid_loader,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "loaders = get_data_loaders(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    \n",
    "    for data_point in batch:\n",
    "        inputs.append(torch.LongTensor(data_point[\"input\"]))\n",
    "        targets.append(torch.LongTensor(data_point[\"target\"]))\n",
    "    \n",
    "    inp_tensor =  torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
    "    target_tensor =  torch.nn.utils.rnn.pad_sequence(targets, batch_first=True)\n",
    "    \n",
    "    return inp_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(target, eos_token=2):\n",
    "    mask = torch.zeros_like(target).to(device)\n",
    "    bz, maxlen = target.shape\n",
    "    for b in range(bz):\n",
    "        for i in range(maxlen):\n",
    "            mask[b, i] = 1\n",
    "            if target[b, i] == eos_token:\n",
    "                break\n",
    "    return mask\n",
    "\n",
    "def calculate_seq_loss(preds, target):\n",
    "    preds = preds\n",
    "    target = target[:, 1:]\n",
    "    assert preds.shape[:2] == target.shape\n",
    "    mask = get_mask(target)\n",
    "\n",
    "    losses = []\n",
    "    for i in range(target.shape[1]):\n",
    "        loss = F.cross_entropy(preds[:, i], target[:, i], reduction='none')\n",
    "        losses.append(loss)\n",
    "    masked_loss = torch.stack(losses, dim=1) * mask\n",
    "    return masked_loss.sum(axis=1).mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderNetwork(nn.Module):\n",
    "    def __init__(self, device=device, ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "    def keep_decoding(self, train, stopped_flag_tensor):\n",
    "        if train:\n",
    "            return True\n",
    "        else:\n",
    "            return not torch.all(stopped_flag_tensor)\n",
    "    \n",
    "    def get_final_inds_tensor(self, hidden):\n",
    "        bz = hidden.shape[0]\n",
    "        return (torch.ones([bz], dtype=torch.long) * -1).to(self.device)\n",
    "    \n",
    "    def get_stopped_flag_tensor(self, hidden):\n",
    "        bz = hidden.shape[0]\n",
    "        return torch.tensor([False] * bz).to(self.device)\n",
    "    \n",
    "    def get_bos_tokens(self, hidden):\n",
    "        bz = hidden.shape[0]\n",
    "        return self.bos_token * torch.ones([bz], dtype=torch.long).to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.input_token2idx = dataset.input_token2idx\n",
    "        self.input_idx2token = {i: char for char, i in self.input_token2idx.items()}\n",
    "        \n",
    "        self.target_token2idx = dataset.target_token2idx\n",
    "        self.target_idx2token = {i: char for char, i in self.target_token2idx.items()}\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def lemmatize_text(self, text):\n",
    "        text, lc, rc = self.prepare_for_lemmatization(text)\n",
    "        inp_list = self.prepare_inference_input(text, lc, rc)\n",
    "        inp_tensor = torch.LongTensor(inp_list).unsqueeze(0)\n",
    "        pred, _, _ = self.forward(inp_tensor)\n",
    "        pred = pred.squeeze()\n",
    "        length = pred.shape[0]\n",
    "        inds = []\n",
    "        \n",
    "        for i in range(length):\n",
    "            cur_pred = torch.argmax(pred[i])\n",
    "            inds.append(cur_pred.item())\n",
    "        return self.stringify(inds, mode=\"target\")\n",
    "        \n",
    "    def stringify(self, inds, mode):\n",
    "        if mode == \"input\":\n",
    "            idx2token = self.input_idx2token\n",
    "        elif mode == \"target\":\n",
    "            idx2token = self.target_idx2token\n",
    "        else:\n",
    "            raise ValueError\n",
    "        chars = []\n",
    "        for ind in inds:\n",
    "            chars.append(idx2token[ind])\n",
    "            if ind == 2:\n",
    "                break\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def prepare_inference_input(self, text, lc, rc):\n",
    "        text = preprocess(text)\n",
    "        lc = preprocess(lc)\n",
    "        rc = preprocess(rc)\n",
    "        \n",
    "        text_tokenized = [self.input_token2idx[char] for char in text]\n",
    "        \n",
    "        lc_tokenized = [self.input_token2idx[char] for char in lc]\n",
    "        lc_border = [self.input_token2idx[\"<lc>\"]]\n",
    "        \n",
    "        rc_tokenized = [self.input_token2idx[char] for char in rc]\n",
    "        rc_border = [self.input_token2idx[\"<rc>\"]]\n",
    "        \n",
    "        return lc_tokenized + lc_border + text_tokenized + rc_border + rc_tokenized\n",
    "    \n",
    "    @staticmethod\n",
    "    def prepare_for_lemmatization(text, sep=\"|\", window_size=25):\n",
    "        lc, target, rc = text.split(sep)\n",
    "        if len(lc) > window_size:\n",
    "            lc = lc[-window_size:]\n",
    "        if len(rc) > window_size:\n",
    "            rc = rc[:window_size]\n",
    "        return target, lc, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_hid_size, emb_dim, emb_count, p, dec_hid_size, num_layers=2, device=device):\n",
    "        self.device = device\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=emb_count,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=enc_hid_size,\n",
    "            dropout=p,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(2 * num_layers * enc_hid_size, dec_hid_size)\n",
    "        \n",
    "    def get_initial_state(self, inp):\n",
    "        shape = self.rnn.get_expected_hidden_size(inp, None)\n",
    "        return torch.zeros(shape).to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        lens = (x != 0).sum(dim=1)\n",
    "        x = self.embeddings(x)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            x, \n",
    "            lengths=lens, \n",
    "            batch_first=True, \n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        states, last_hidden = self.rnn(packed, self.get_initial_state(x))\n",
    "        states, lens = torch.nn.utils.rnn.pad_packed_sequence(states, batch_first=True)\n",
    "        last_hidden = torch.cat([*last_hidden], axis=1)\n",
    "        \n",
    "        last_hidden = self.fc(last_hidden)\n",
    "        last_hidden = torch.tanh(last_hidden)\n",
    "        last_hidden = self.dropout(last_hidden)\n",
    "        \n",
    "        return (states, lens), last_hidden\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder_hid_size, decoder_hid_size, attn_units):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(2 * encoder_hid_size + decoder_hid_size, attn_units)\n",
    "        self.V = nn.Linear(attn_units, 1)\n",
    "        \n",
    "        self.encoder_states = None\n",
    "        self.enc_seq_len = None\n",
    "        \n",
    "    def get_scores(self, concated_states):\n",
    "        return self.V(torch.tanh(self.attn(concated_states))).squeeze()\n",
    "        \n",
    "    def init_states(self, encoder_states):\n",
    "        self.encoder_states = encoder_states\n",
    "        self.encoder_mask = (encoder_states.sum(dim=2) != 0).float()\n",
    "        self.enc_seq_len = encoder_states.shape[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def masked_softmax(tensor, mask, dim=1):\n",
    "        exps = torch.exp(tensor)\n",
    "        exps = exps * mask\n",
    "        divider = torch.sum(exps, keepdim=True, dim=dim)\n",
    "        return exps / divider\n",
    "        \n",
    "    def calc_attention(self, dec_hidden):\n",
    "        \n",
    "        expanded_dec_hidden = dec_hidden.unsqueeze(1).repeat(1, self.enc_seq_len, 1)\n",
    "        \n",
    "        concated_states = torch.cat([self.encoder_states, expanded_dec_hidden], dim=2)\n",
    "        scores = self.get_scores(concated_states)\n",
    "        \n",
    "        weights = self.masked_softmax(scores, self.encoder_mask)\n",
    "        \n",
    "        attn_vecs = (self.encoder_states * weights.unsqueeze(2)).sum(dim=1)\n",
    "        return attn_vecs, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalDecoder(DecoderNetwork):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 enc_hid_dim,\n",
    "                 dec_hid_dim,\n",
    "                 emb_dim,\n",
    "                 emb_count, \n",
    "                 attn_dim, \n",
    "                 maxlen=30,\n",
    "                 p=0.2, \n",
    "                 eos_token=2,\n",
    "                 bos_token=1,\n",
    "                 teacher_forcing_rate=0.0,\n",
    "                 device=device):\n",
    "        super().__init__(device)\n",
    "        self.embeddings = nn.Embedding(\n",
    "            embedding_dim=emb_dim,\n",
    "            num_embeddings=emb_count,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "        \n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "        self.rnn_cell_bottom = nn.GRUCell(emb_dim, dec_hid_dim)\n",
    "        self.attn_module = BahdanauAttention(enc_hid_dim, dec_hid_dim, attn_dim)\n",
    "        self.rnn_cell_top = nn.GRUCell(dec_hid_dim, enc_hid_dim * 2)\n",
    "        \n",
    "        self.recurrent_fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.teacher_forcing_rate = teacher_forcing_rate\n",
    "        self.output_proj = nn.Linear(dec_hid_dim, emb_count)\n",
    "        \n",
    "        self.recurrent_fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def process_input(self, inputs, state):\n",
    "        embedded = self.embeddings(inputs)\n",
    "        step1 = self.rnn_cell_bottom(embedded, state)\n",
    "        step1 = self.dropout(step1)\n",
    "        \n",
    "        attn_vec, weights = self.attn_module.calc_attention(step1)\n",
    "        attn_vec = self.dropout(attn_vec)\n",
    "        \n",
    "        step2 = self.rnn_cell_top(step1, attn_vec)\n",
    "        \n",
    "        step2 = self.relu(step2)\n",
    "        step2 = self.recurrent_fc(step2)\n",
    "\n",
    "        return step2, weights\n",
    "        \n",
    "    \n",
    "    def decode(self, hidden, enc_states, true_labels=None, train=False):\n",
    "        assert (train and true_labels is not None) or (not train and true_labels is None)\n",
    "        self.attn_module.init_states(enc_states)\n",
    "        \n",
    "        inputs = self.get_bos_tokens(hidden)\n",
    "        \n",
    "        state, weights = self.process_input(inputs, hidden)\n",
    "        attn_weights = [weights]\n",
    "        states = [state]\n",
    "        \n",
    "        final_inds_tensor = self.get_final_inds_tensor(hidden)\n",
    "        stopped_flag_tensor = self.get_stopped_flag_tensor(hidden)\n",
    "        \n",
    "        steps = 1\n",
    "        \n",
    "        if train:\n",
    "            maxlen = true_labels.shape[1] - 1\n",
    "        else:\n",
    "            maxlen = self.maxlen\n",
    "        while steps < maxlen and self.keep_decoding(train, stopped_flag_tensor):\n",
    "            steps += 1\n",
    "            if train and torch.rand(1) < self.teacher_forcing_rate:\n",
    "                inputs = true_labels[:, steps]\n",
    "            else:\n",
    "                output_proj = self.output_proj(state)\n",
    "                inputs = torch.argmax(output_proj, dim=1)\n",
    "            state, weights = self.process_input(inputs, state)\n",
    "            attn_weights.append(weights)\n",
    "            states.append(state)\n",
    "            \n",
    "            stopped_flag_tensor = stopped_flag_tensor | (inputs == self.eos_token)\n",
    "            final_inds_tensor = torch.where(\n",
    "                stopped_flag_tensor & (final_inds_tensor == -1),\n",
    "                torch.tensor(steps).to(self.device),\n",
    "                final_inds_tensor,\n",
    "            )\n",
    "        final_inds_tensor = torch.where(final_inds_tensor == -1, \n",
    "                                        torch.tensor(steps).to(self.device),\n",
    "                                        final_inds_tensor)\n",
    "        \n",
    "        states = torch.stack(states, dim=1)\n",
    "        attn_weights = torch.stack(attn_weights, dim=1)\n",
    "        preds = self.output_proj(states)\n",
    "        return preds, final_inds_tensor, attn_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSeq2Seq(Seq2Seq):\n",
    "    \n",
    "    def __init__(self, enc_params, dec_params, device=device):\n",
    "        super().__init__(device)\n",
    "        self.encoder = EncoderNetwork(**enc_params)\n",
    "        self.decoder = ConditionalDecoder(**dec_params)\n",
    "        \n",
    "    def forward(self, x, true_labels=None, train=False):\n",
    "        x = x.to(device)\n",
    "        if true_labels is not None:\n",
    "            true_labels = true_labels.to(device)\n",
    "        states, last_hidden =  self.encoder(x)\n",
    "        preds, lens, attn_weights = self.decoder.decode(last_hidden, states[0], true_labels, train)\n",
    "        return preds, lens, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpad_seq(batch_inds, eos_token=2):\n",
    "    result = []\n",
    "    cur_seq = []\n",
    "    for point in batch_inds:\n",
    "        for ind in point:\n",
    "            if ind != 2:\n",
    "                cur_seq.append(ind.item())\n",
    "            else:\n",
    "                break\n",
    "        result.append(cur_seq)\n",
    "        cur_seq = []\n",
    "    return result\n",
    "\n",
    "\n",
    "def calc_accuracy(preds, target):\n",
    "    pred_inds = preds.argmax(dim=2)\n",
    "    target = target[:, 1:]\n",
    "    pred_seqs = unpad_seq(pred_inds)\n",
    "    true_seqs = unpad_seq(target)\n",
    "    goods = 0\n",
    "    for y_pred, y_true in zip(pred_seqs, true_seqs):\n",
    "        if y_pred == y_true:\n",
    "            goods += 1\n",
    "    return goods / pred_inds.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for i, batch in enumerate(loaders['valid']):\n",
    "        inp, target = batch\n",
    "        target = target.to(device)\n",
    "        preds, lens, _ = model(inp, true_labels=target, train=True)\n",
    "            \n",
    "        loss = calculate_seq_loss(preds, target)\n",
    "        losses.append(loss.item())\n",
    "            \n",
    "        acc = calc_accuracy(preds, target)\n",
    "        accs.append(acc)\n",
    "    return sum(losses) / len(losses), sum(accs) / len(accs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, NUM_EPOCHS=1, clip_value=1, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.15)\n",
    "    for k in range(NUM_EPOCHS):\n",
    "        print(f\"Starting epoch {k}\")\n",
    "        model.train()\n",
    "        losses = []\n",
    "        accs = []\n",
    "        for i, batch in enumerate(loaders['train']):\n",
    "            optimizer.zero_grad()\n",
    "            inp, target = batch\n",
    "            target = target.to(device)\n",
    "            preds, lens, _ = model(inp, true_labels=target, train=True)\n",
    "            \n",
    "            loss = calculate_seq_loss(preds, target)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            acc = calc_accuracy(preds, target)\n",
    "            accs.append(acc)            \n",
    "            \n",
    "            loss.backward()            \n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"train loss step {i} = {sum(losses) / len(losses)}\")\n",
    "                print(f\"train accs step {i} = {sum(accs) / len(accs)}\")\n",
    "                print()\n",
    "                losses = []\n",
    "                accs = []\n",
    "        valid_loss, valid_acc = validate_model(model)\n",
    "        print(f\"valid loss epoch {k} = {valid_loss}\")\n",
    "        print(f\"valid accs epoch {k} = {valid_acc}\")\n",
    "        lr_scheduler.step()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_params = {\n",
    "    \"enc_hid_size\": 256, \n",
    "    \"dec_hid_size\": 256,\n",
    "    \"emb_dim\": 200,\n",
    "    \"emb_count\": len(dataset.input_token2idx),\n",
    "    \"num_layers\": 2,\n",
    "    \"p\": 0.2, \n",
    "}\n",
    "\n",
    "cond_dec_params = {\n",
    "    \"enc_hid_dim\": 256,\n",
    "    \"dec_hid_dim\": 256,\n",
    "    \"emb_dim\": 200, \n",
    "    \"emb_count\": len(dataset.target_token2idx),\n",
    "    \"attn_dim\": 100,\n",
    "    \"p\": 0.2,\n",
    "    \"teacher_forcing_rate\": 0.4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_model = ConditionalSeq2Seq(enc_params, cond_dec_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "train loss step 0 = 29.791912078857422\n",
      "train accs step 0 = 0.0\n",
      "\n",
      "train loss step 100 = 17.82345296859741\n",
      "train accs step 100 = 0.0071875\n",
      "\n",
      "train loss step 200 = 12.426255202293396\n",
      "train accs step 200 = 0.09875\n",
      "\n",
      "train loss step 300 = 5.477972460985184\n",
      "train accs step 300 = 0.4196875\n",
      "\n",
      "train loss step 400 = 3.701076601743698\n",
      "train accs step 400 = 0.55015625\n",
      "\n",
      "train loss step 500 = 2.769656639099121\n",
      "train accs step 500 = 0.64515625\n",
      "\n",
      "train loss step 600 = 2.2979648685455323\n",
      "train accs step 600 = 0.698984375\n",
      "\n",
      "train loss step 700 = 1.9486912441253663\n",
      "train accs step 700 = 0.730703125\n",
      "\n",
      "train loss step 800 = 1.8189603686332703\n",
      "train accs step 800 = 0.75828125\n",
      "\n",
      "train loss step 900 = 1.4886805638670921\n",
      "train accs step 900 = 0.77859375\n",
      "\n",
      "train loss step 1000 = 1.8013139227032662\n",
      "train accs step 1000 = 0.771875\n",
      "\n",
      "train loss step 1100 = 1.4700160697102547\n",
      "train accs step 1100 = 0.794140625\n",
      "\n",
      "train loss step 1200 = 1.4347372949123383\n",
      "train accs step 1200 = 0.796953125\n",
      "\n",
      "train loss step 1300 = 1.2613818873465061\n",
      "train accs step 1300 = 0.8184375\n",
      "\n",
      "train loss step 1400 = 1.3087276935577392\n",
      "train accs step 1400 = 0.814765625\n",
      "\n",
      "train loss step 1500 = 1.1399729639291762\n",
      "train accs step 1500 = 0.831953125\n",
      "\n",
      "train loss step 1600 = 1.201202199459076\n",
      "train accs step 1600 = 0.830078125\n",
      "\n",
      "train loss step 1700 = 1.1243095031380654\n",
      "train accs step 1700 = 0.842421875\n",
      "\n",
      "train loss step 1800 = 0.9474145737290383\n",
      "train accs step 1800 = 0.847421875\n",
      "\n",
      "train loss step 1900 = 1.1098346100747585\n",
      "train accs step 1900 = 0.838984375\n",
      "\n",
      "train loss step 2000 = 1.0323901753127576\n",
      "train accs step 2000 = 0.84328125\n",
      "\n",
      "train loss step 2100 = 0.9977444273233413\n",
      "train accs step 2100 = 0.847265625\n",
      "\n",
      "train loss step 2200 = 1.023203613758087\n",
      "train accs step 2200 = 0.848203125\n",
      "\n",
      "train loss step 2300 = 0.9446250739693641\n",
      "train accs step 2300 = 0.851953125\n",
      "\n",
      "train loss step 2400 = 0.9758511903882027\n",
      "train accs step 2400 = 0.854375\n",
      "\n",
      "train loss step 2500 = 0.9246557746827603\n",
      "train accs step 2500 = 0.86140625\n",
      "\n",
      "train loss step 2600 = 0.9540935760736465\n",
      "train accs step 2600 = 0.855\n",
      "\n",
      "train loss step 2700 = 0.8516358329355717\n",
      "train accs step 2700 = 0.86625\n",
      "\n",
      "train loss step 2800 = 0.8259500885009765\n",
      "train accs step 2800 = 0.868203125\n",
      "\n",
      "train loss step 2900 = 0.8845079752802849\n",
      "train accs step 2900 = 0.864765625\n",
      "\n",
      "train loss step 3000 = 0.877731713950634\n",
      "train accs step 3000 = 0.869453125\n",
      "\n",
      "train loss step 3100 = 0.9049561505019664\n",
      "train accs step 3100 = 0.8646875\n",
      "\n",
      "train loss step 3200 = 0.786661474108696\n",
      "train accs step 3200 = 0.8771875\n",
      "\n",
      "train loss step 3300 = 0.8100552377104759\n",
      "train accs step 3300 = 0.875546875\n",
      "\n",
      "train loss step 3400 = 0.7856060272455215\n",
      "train accs step 3400 = 0.875234375\n",
      "\n",
      "train loss step 3500 = 0.7753930798172951\n",
      "train accs step 3500 = 0.87984375\n",
      "\n",
      "train loss step 3600 = 0.9349279980361461\n",
      "train accs step 3600 = 0.8734375\n",
      "\n",
      "train loss step 3700 = 0.8238534858822822\n",
      "train accs step 3700 = 0.877578125\n",
      "\n",
      "train loss step 3800 = 0.7765645802021026\n",
      "train accs step 3800 = 0.87890625\n",
      "\n",
      "train loss step 3900 = 0.7267084503173828\n",
      "train accs step 3900 = 0.881796875\n",
      "\n",
      "train loss step 4000 = 0.8452391438186169\n",
      "train accs step 4000 = 0.879609375\n",
      "\n",
      "valid loss epoch 0 = 0.5872640454753784\n",
      "valid accs epoch 0 = 0.8928945310987181\n",
      "Starting epoch 1\n",
      "train loss step 0 = 0.5362188220024109\n",
      "train accs step 0 = 0.8984375\n",
      "\n",
      "train loss step 100 = 0.6059905445575714\n",
      "train accs step 100 = 0.8934375\n",
      "\n",
      "train loss step 200 = 0.6122238072752952\n",
      "train accs step 200 = 0.896953125\n",
      "\n",
      "train loss step 300 = 0.5769565208256244\n",
      "train accs step 300 = 0.90359375\n",
      "\n",
      "train loss step 400 = 0.540262876600027\n",
      "train accs step 400 = 0.9040625\n",
      "\n",
      "train loss step 500 = 0.4495610421150923\n",
      "train accs step 500 = 0.9109375\n",
      "\n",
      "train loss step 600 = 0.5173654751479626\n",
      "train accs step 600 = 0.9165625\n",
      "\n",
      "train loss step 700 = 0.5394807544350624\n",
      "train accs step 700 = 0.905859375\n",
      "\n",
      "train loss step 800 = 0.49185751505196096\n",
      "train accs step 800 = 0.91203125\n",
      "\n",
      "train loss step 900 = 0.4724452303349972\n",
      "train accs step 900 = 0.91265625\n",
      "\n",
      "train loss step 1000 = 0.4576922545582056\n",
      "train accs step 1000 = 0.914765625\n",
      "\n",
      "train loss step 1100 = 0.4170628367364407\n",
      "train accs step 1100 = 0.923203125\n",
      "\n",
      "train loss step 1200 = 0.4448514068871737\n",
      "train accs step 1200 = 0.916171875\n",
      "\n",
      "train loss step 1300 = 0.44063251703977585\n",
      "train accs step 1300 = 0.920546875\n",
      "\n",
      "train loss step 1400 = 0.4255216544866562\n",
      "train accs step 1400 = 0.921171875\n",
      "\n",
      "train loss step 1500 = 0.44802285626530647\n",
      "train accs step 1500 = 0.91671875\n",
      "\n",
      "train loss step 1600 = 0.4209880922734737\n",
      "train accs step 1600 = 0.9265625\n",
      "\n",
      "train loss step 1700 = 0.4092720827460289\n",
      "train accs step 1700 = 0.920546875\n",
      "\n",
      "train loss step 1800 = 0.39103118181228635\n",
      "train accs step 1800 = 0.9290625\n",
      "\n",
      "train loss step 1900 = 0.4082789515703917\n",
      "train accs step 1900 = 0.921484375\n",
      "\n",
      "train loss step 2000 = 0.3897792053967714\n",
      "train accs step 2000 = 0.927578125\n",
      "\n",
      "train loss step 2100 = 0.38087260887026786\n",
      "train accs step 2100 = 0.92828125\n",
      "\n",
      "train loss step 2200 = 0.3975043924897909\n",
      "train accs step 2200 = 0.925625\n",
      "\n",
      "train loss step 2300 = 0.40587194107472896\n",
      "train accs step 2300 = 0.923671875\n",
      "\n",
      "train loss step 2400 = 0.3985351523011923\n",
      "train accs step 2400 = 0.928671875\n",
      "\n",
      "train loss step 2500 = 0.45059061743319034\n",
      "train accs step 2500 = 0.923046875\n",
      "\n",
      "train loss step 2600 = 0.36766418419778346\n",
      "train accs step 2600 = 0.928203125\n",
      "\n",
      "train loss step 2700 = 0.36107448950409887\n",
      "train accs step 2700 = 0.929140625\n",
      "\n",
      "train loss step 2800 = 0.40441451281309126\n",
      "train accs step 2800 = 0.928359375\n",
      "\n",
      "train loss step 2900 = 0.352337157279253\n",
      "train accs step 2900 = 0.926796875\n",
      "\n",
      "train loss step 3000 = 0.3528495440632105\n",
      "train accs step 3000 = 0.9284375\n",
      "\n",
      "train loss step 3100 = 0.40705122508108615\n",
      "train accs step 3100 = 0.92640625\n",
      "\n",
      "train loss step 3200 = 0.3663292756676674\n",
      "train accs step 3200 = 0.926953125\n",
      "\n",
      "train loss step 3300 = 0.3862091800943017\n",
      "train accs step 3300 = 0.931171875\n",
      "\n",
      "train loss step 3400 = 0.37530858658254146\n",
      "train accs step 3400 = 0.928359375\n",
      "\n",
      "train loss step 3500 = 0.38367542646825314\n",
      "train accs step 3500 = 0.92984375\n",
      "\n",
      "train loss step 3600 = 0.3571082197874784\n",
      "train accs step 3600 = 0.933984375\n",
      "\n",
      "train loss step 3700 = 0.34949351646006105\n",
      "train accs step 3700 = 0.934921875\n",
      "\n",
      "train loss step 3800 = 0.332760998159647\n",
      "train accs step 3800 = 0.93578125\n",
      "\n",
      "train loss step 3900 = 0.3222039794176817\n",
      "train accs step 3900 = 0.93546875\n",
      "\n",
      "train loss step 4000 = 0.33061517640948296\n",
      "train accs step 4000 = 0.933828125\n",
      "\n",
      "valid loss epoch 1 = 0.27244335239735307\n",
      "valid accs epoch 1 = 0.9461533248131366\n",
      "Starting epoch 2\n",
      "train loss step 0 = 0.22835206985473633\n",
      "train accs step 0 = 0.9375\n",
      "\n",
      "train loss step 100 = 0.3368569227680564\n",
      "train accs step 100 = 0.93640625\n",
      "\n",
      "train loss step 200 = 0.3146330900862813\n",
      "train accs step 200 = 0.93703125\n",
      "\n",
      "train loss step 300 = 0.3180705953389406\n",
      "train accs step 300 = 0.93578125\n",
      "\n",
      "train loss step 400 = 0.3227429424971342\n",
      "train accs step 400 = 0.933984375\n",
      "\n",
      "train loss step 500 = 0.2978553430736065\n",
      "train accs step 500 = 0.936796875\n",
      "\n",
      "train loss step 600 = 0.3019798221066594\n",
      "train accs step 600 = 0.941328125\n",
      "\n",
      "train loss step 700 = 0.29387883737683296\n",
      "train accs step 700 = 0.94046875\n",
      "\n",
      "train loss step 800 = 0.3222757728025317\n",
      "train accs step 800 = 0.938984375\n",
      "\n",
      "train loss step 900 = 0.3028603196144104\n",
      "train accs step 900 = 0.940859375\n",
      "\n",
      "train loss step 1000 = 0.30623600019142033\n",
      "train accs step 1000 = 0.940078125\n",
      "\n",
      "train loss step 1100 = 0.3046725475788116\n",
      "train accs step 1100 = 0.941484375\n",
      "\n",
      "train loss step 1200 = 0.3120506066083908\n",
      "train accs step 1200 = 0.93859375\n",
      "\n",
      "train loss step 1300 = 0.2996184199303389\n",
      "train accs step 1300 = 0.93859375\n",
      "\n",
      "train loss step 1400 = 0.28738135263323783\n",
      "train accs step 1400 = 0.94484375\n",
      "\n",
      "train loss step 1500 = 0.32268507886677983\n",
      "train accs step 1500 = 0.93859375\n",
      "\n",
      "train loss step 1600 = 0.26658718384802343\n",
      "train accs step 1600 = 0.94328125\n",
      "\n",
      "train loss step 1700 = 0.29145021811127664\n",
      "train accs step 1700 = 0.938671875\n",
      "\n",
      "train loss step 1800 = 0.34513789024204017\n",
      "train accs step 1800 = 0.9384375\n",
      "\n",
      "train loss step 1900 = 0.3422466445714235\n",
      "train accs step 1900 = 0.939609375\n",
      "\n",
      "train loss step 2000 = 0.2669896226376295\n",
      "train accs step 2000 = 0.94078125\n",
      "\n",
      "train loss step 2100 = 0.31506856337189676\n",
      "train accs step 2100 = 0.936484375\n",
      "\n",
      "train loss step 2200 = 0.34817087050527334\n",
      "train accs step 2200 = 0.940234375\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss step 2300 = 0.30351193368434903\n",
      "train accs step 2300 = 0.944375\n",
      "\n",
      "train loss step 2400 = 0.30785492226481437\n",
      "train accs step 2400 = 0.944375\n",
      "\n",
      "train loss step 2500 = 0.3070417229086161\n",
      "train accs step 2500 = 0.938046875\n",
      "\n",
      "train loss step 2600 = 0.29593746431171897\n",
      "train accs step 2600 = 0.939375\n",
      "\n",
      "train loss step 2700 = 0.31732655052095654\n",
      "train accs step 2700 = 0.94015625\n",
      "\n",
      "train loss step 2800 = 0.28621554709970953\n",
      "train accs step 2800 = 0.9425\n",
      "\n",
      "train loss step 2900 = 0.2961538009345531\n",
      "train accs step 2900 = 0.941640625\n",
      "\n",
      "train loss step 3000 = 0.343190878406167\n",
      "train accs step 3000 = 0.94234375\n",
      "\n",
      "train loss step 3100 = 0.28141997385770084\n",
      "train accs step 3100 = 0.941015625\n",
      "\n",
      "train loss step 3200 = 0.2930500101670623\n",
      "train accs step 3200 = 0.94125\n",
      "\n",
      "train loss step 3300 = 0.3049010809883475\n",
      "train accs step 3300 = 0.939765625\n",
      "\n",
      "train loss step 3400 = 0.2898872723430395\n",
      "train accs step 3400 = 0.942421875\n",
      "\n",
      "train loss step 3500 = 0.30787438694387675\n",
      "train accs step 3500 = 0.938828125\n",
      "\n",
      "train loss step 3600 = 0.2872187675535679\n",
      "train accs step 3600 = 0.9440625\n",
      "\n",
      "train loss step 3700 = 0.3313024341315031\n",
      "train accs step 3700 = 0.935859375\n",
      "\n",
      "train loss step 3800 = 0.3177708100154996\n",
      "train accs step 3800 = 0.93953125\n",
      "\n",
      "train loss step 3900 = 0.2919746273756027\n",
      "train accs step 3900 = 0.94203125\n",
      "\n",
      "train loss step 4000 = 0.28930669754743576\n",
      "train accs step 4000 = 0.942890625\n",
      "\n",
      "valid loss epoch 2 = 0.23479589447653232\n",
      "valid accs epoch 2 = 0.9521104027083899\n",
      "Starting epoch 3\n",
      "train loss step 0 = 0.22054284811019897\n",
      "train accs step 0 = 0.9375\n",
      "\n",
      "train loss step 100 = 0.35421445831656456\n",
      "train accs step 100 = 0.93703125\n",
      "\n",
      "train loss step 200 = 0.24959136303514243\n",
      "train accs step 200 = 0.9428125\n",
      "\n",
      "train loss step 300 = 0.28670636292546986\n",
      "train accs step 300 = 0.94125\n",
      "\n",
      "train loss step 400 = 0.2993717931583524\n",
      "train accs step 400 = 0.942578125\n",
      "\n",
      "train loss step 500 = 0.27163476377725604\n",
      "train accs step 500 = 0.94078125\n",
      "\n",
      "train loss step 600 = 0.3045933327078819\n",
      "train accs step 600 = 0.937265625\n",
      "\n",
      "train loss step 700 = 0.25729336827993393\n",
      "train accs step 700 = 0.94390625\n",
      "\n",
      "train loss step 800 = 0.31030970722436907\n",
      "train accs step 800 = 0.93984375\n",
      "\n",
      "train loss step 900 = 0.30337925028055907\n",
      "train accs step 900 = 0.941953125\n",
      "\n",
      "train loss step 1000 = 0.3162718493491411\n",
      "train accs step 1000 = 0.9434375\n",
      "\n",
      "train loss step 1100 = 0.27692716708406806\n",
      "train accs step 1100 = 0.94703125\n",
      "\n",
      "train loss step 1200 = 0.29027586890384555\n",
      "train accs step 1200 = 0.940859375\n",
      "\n",
      "train loss step 1300 = 0.2832773258537054\n",
      "train accs step 1300 = 0.942421875\n",
      "\n",
      "train loss step 1400 = 0.30004107411950826\n",
      "train accs step 1400 = 0.939609375\n",
      "\n",
      "train loss step 1500 = 0.3271761691942811\n",
      "train accs step 1500 = 0.941640625\n",
      "\n",
      "train loss step 1600 = 0.27236944548785685\n",
      "train accs step 1600 = 0.94359375\n",
      "\n",
      "train loss step 1700 = 0.28930792700499297\n",
      "train accs step 1700 = 0.939609375\n",
      "\n",
      "train loss step 1800 = 0.29466710433363913\n",
      "train accs step 1800 = 0.94140625\n",
      "\n",
      "train loss step 1900 = 0.2749968255311251\n",
      "train accs step 1900 = 0.942265625\n",
      "\n",
      "train loss step 2000 = 0.3003577882796526\n",
      "train accs step 2000 = 0.94375\n",
      "\n",
      "train loss step 2100 = 0.29003065373748543\n",
      "train accs step 2100 = 0.938125\n",
      "\n",
      "train loss step 2200 = 0.2423531411588192\n",
      "train accs step 2200 = 0.946171875\n",
      "\n",
      "train loss step 2300 = 0.30439287699759005\n",
      "train accs step 2300 = 0.939375\n",
      "\n",
      "train loss step 2400 = 0.2822810657694936\n",
      "train accs step 2400 = 0.9415625\n",
      "\n",
      "train loss step 2500 = 0.2628209645487368\n",
      "train accs step 2500 = 0.942890625\n",
      "\n",
      "train loss step 2600 = 0.3004810844361782\n",
      "train accs step 2600 = 0.940859375\n",
      "\n",
      "train loss step 2700 = 0.27689416248351334\n",
      "train accs step 2700 = 0.94296875\n",
      "\n",
      "train loss step 2800 = 0.2829921484366059\n",
      "train accs step 2800 = 0.94328125\n",
      "\n",
      "train loss step 2900 = 0.2786688717082143\n",
      "train accs step 2900 = 0.941796875\n",
      "\n",
      "train loss step 3000 = 0.2729900714196265\n",
      "train accs step 3000 = 0.942421875\n",
      "\n",
      "train loss step 3100 = 0.2653978913277388\n",
      "train accs step 3100 = 0.945390625\n",
      "\n",
      "train loss step 3200 = 0.271883234642446\n",
      "train accs step 3200 = 0.943515625\n",
      "\n",
      "train loss step 3300 = 0.3063873882219195\n",
      "train accs step 3300 = 0.938515625\n",
      "\n",
      "train loss step 3400 = 0.28332439785823227\n",
      "train accs step 3400 = 0.945234375\n",
      "\n",
      "train loss step 3500 = 0.2655555411055684\n",
      "train accs step 3500 = 0.946875\n",
      "\n",
      "train loss step 3600 = 0.2940981232374906\n",
      "train accs step 3600 = 0.9425\n",
      "\n",
      "train loss step 3700 = 0.2959904485568404\n",
      "train accs step 3700 = 0.94140625\n",
      "\n",
      "train loss step 3800 = 0.28817173175513744\n",
      "train accs step 3800 = 0.943828125\n",
      "\n",
      "train loss step 3900 = 0.2910775760561228\n",
      "train accs step 3900 = 0.940859375\n",
      "\n",
      "train loss step 4000 = 0.30336065508425236\n",
      "train accs step 4000 = 0.93859375\n",
      "\n",
      "valid loss epoch 3 = 0.23568008864823403\n",
      "valid accs epoch 3 = 0.9527269268876755\n"
     ]
    }
   ],
   "source": [
    "trained_cond_model = train_model(cond_model, NUM_EPOCHS=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'думать<EOS>'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.lemmatize_text(\"привет почему ты так |думаешь|?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = validate_model(teacher_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533435519215626"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2426984647249171"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
